{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8cf7eae",
   "metadata": {},
   "source": [
    "# model with train 716 pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fe2a3d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ultralytics import YOLO\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a95843",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ============ Configuración ============\n",
    "# Ruta al JSON exportado de Label Studio (formato Common Raw JSON)\n",
    "LABEL_JSON    = r'C:\\Users\\juans\\Documents\\proarchitecg\\version_2_docker\\model_clasification_image_v2\\train.json'\n",
    "\n",
    "# Carpeta donde están las imágenes originales\n",
    "IMAGES_ROOT   = r'C:\\Users\\juans\\Documents\\proarchitecg\\version_2_docker\\model_clasification_image_v2\\images'\n",
    "\n",
    "# Carpeta de salida (train/val preparado)\n",
    "OUTPUT_ROOT   = r'C:\\Users\\juans\\Documents\\proarchitecg\\version_2_docker\\model_clasification_image_v2\\dataset_cls'\n",
    "\n",
    "VAL_SPLIT     = 0.2\n",
    "PRETRAINED    = 'yolov8n-cls.pt'\n",
    "EPOCHS        = 50\n",
    "BATCH_SIZE    = 16\n",
    "IMG_SIZE      = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c11c6e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# =======================================\n",
    "\n",
    "def load_annotations(label_json_path):\n",
    "    \"\"\"\n",
    "    Lee export.json de Label Studio y devuelve lista de tuplas (image_path, label_name).\n",
    "    Asume que en cada item:\n",
    "      - item['data']['image'] = ruta o filename de la imagen\n",
    "      - item['annotations'][0]['result'][0]['value']['choices'][0] = nombre de la clase\n",
    "    Ajusta según tu estructura exacta.\n",
    "    \"\"\"\n",
    "    with open(label_json_path, encoding='utf-8') as f:\n",
    "        raw = json.load(f)\n",
    "\n",
    "    samples = []\n",
    "    for item in raw:\n",
    "        # Ruta de imagen en el campo data.image\n",
    "        img_ref = item['data'].get('image')\n",
    "        # Nombre de la imagen (basename)\n",
    "        img_name = os.path.basename(img_ref)\n",
    "        # Clase anotada (primer annotation → primer result → choices[0])\n",
    "        ann = item.get('annotations') or item.get('predictions')\n",
    "        if not ann:\n",
    "            continue\n",
    "        result = ann[0].get('result') if isinstance(ann[0], dict) else None\n",
    "        if not result or not isinstance(result, list) or len(result) == 0:\n",
    "            continue\n",
    "        value = result[0].get('value') if isinstance(result[0], dict) else None\n",
    "        if not value or 'choices' not in value or not value['choices']:\n",
    "            continue\n",
    "        label = value['choices'][0]\n",
    "        samples.append((img_name, label))\n",
    "    return samples\n",
    "\n",
    "def prepare_dataset_from_labels():\n",
    "    # 1) Cargar anotaciones\n",
    "    samples = load_annotations(LABEL_JSON)\n",
    "    # 2) Agrupar por etiqueta\n",
    "    from collections import defaultdict\n",
    "    by_label = defaultdict(list)\n",
    "    for img_name, label in samples:\n",
    "        by_label[label].append(img_name)\n",
    "\n",
    "    # 3) Separar singleton vs multies\n",
    "    multi, single = [], []\n",
    "    for label, imgs in by_label.items():\n",
    "        if len(imgs) > 1:\n",
    "            multi += [(img, label) for img in imgs]\n",
    "        else:\n",
    "            single += [(imgs[0], label)]\n",
    "\n",
    "    # 4) Estratified split solo sobre multi\n",
    "    if multi:\n",
    "        train_m, val_m = train_test_split(\n",
    "            multi,\n",
    "            test_size=VAL_SPLIT,\n",
    "            stratify=[lbl for _,lbl in multi],\n",
    "            random_state=42\n",
    "        )\n",
    "    else:\n",
    "        train_m, val_m = [], []\n",
    "\n",
    "    # 5) Todos los single van a train\n",
    "    train_samples = train_m + single\n",
    "    val_samples   = val_m\n",
    "\n",
    "    # 6) Limpiar y copiar\n",
    "    if os.path.exists(OUTPUT_ROOT):\n",
    "        shutil.rmtree(OUTPUT_ROOT)\n",
    "    os.makedirs(OUTPUT_ROOT)\n",
    "    def copy_split(split, split_name):\n",
    "        for img_name, label in split:\n",
    "            src = os.path.join(IMAGES_ROOT, img_name)\n",
    "            dst_dir = os.path.join(OUTPUT_ROOT, split_name, label)\n",
    "            os.makedirs(dst_dir, exist_ok=True)\n",
    "            dst = os.path.join(dst_dir, img_name)\n",
    "            if os.path.exists(src) and not os.path.exists(dst):\n",
    "                shutil.copy(src, dst)\n",
    "\n",
    "    copy_split(train_samples, 'train')\n",
    "    copy_split(val_samples,   'val')\n",
    "\n",
    "    # 7) Generar data.yaml\n",
    "    labels = sorted(by_label.keys())\n",
    "    data_dict = {\n",
    "        'path': OUTPUT_ROOT,  \n",
    "        'train': 'train',\n",
    "        'val':   'val',\n",
    "        'nc':    len(labels),\n",
    "        'names': labels\n",
    "    }\n",
    "    with open(os.path.join(OUTPUT_ROOT, 'data.yaml'), 'w') as f:\n",
    "        yaml.dump(data_dict, f, sort_keys=False)\n",
    "\n",
    "    print(f\"Dataset preparado:\")\n",
    "    print(f\" • Clases totales: {labels}\")\n",
    "    print(f\" • Train: {len(train_samples)} imágenes\")\n",
    "    print(f\" • Val:   {len(val_samples)} imágenes\")\n",
    "    print(f\" • data.yaml generado en {OUTPUT_ROOT}\")\n",
    "def train_model():\n",
    "    # En lugar de pasar data=yaml_path, pasamos OUTPUT_ROOT\n",
    "    model = YOLO(PRETRAINED)\n",
    "    model.train(\n",
    "        data=OUTPUT_ROOT,   # <-- Carpeta que contiene data.yaml, train/ y val/\n",
    "        epochs=EPOCHS,\n",
    "        imgsz=IMG_SIZE,\n",
    "        batch=BATCH_SIZE,\n",
    "        name='person_cls'\n",
    "    )\n",
    "    print(\"✅ Entrenamiento finalizado. Revisa runs/classify/person_cls\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    prepare_dataset_from_labels()\n",
    "    train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ee5686",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# end to entrain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767279a5",
   "metadata": {},
   "source": [
    "# script clasification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424f1ad0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import json\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "from rapidfuzz import fuzz\n",
    "from ultralytics import YOLO\n",
    "from openpyxl import load_workbook\n",
    "from copy import copy\n",
    "from doc_type_patterns import OCR_PATTERNS\n",
    "from openpyxl.utils import get_column_letter\n",
    "from openpyxl.styles.borders import Border, Side\n",
    "from openpyxl.utils import get_column_letter, range_boundaries\n",
    "import time\n",
    "\n",
    "# ———RUTAS y UMBRALES ———\n",
    "MODEL_PATH      = r'C:\\Users\\juans\\Documents\\proarchitecg\\version_2_docker\\model_clasification_image_v2\\runs\\classify\\person_cls\\weights\\best.pt'\n",
    "IMAGES_ROOT     = r'C:\\Users\\juans\\Documents\\proarchitecg\\version_2_docker\\imagenes_por_doc'\n",
    "OCR_ROOT        = r'C:\\Users\\juans\\Documents\\proarchitecg\\version_2_docker\\ocr_por_doc'\n",
    "OUTPUT_FILE     = r'C:\\Users\\juans\\Documents\\proarchitecg\\version_2_docker\\resultados_completos_v_final.xlsx'\n",
    "TEMPLATE_PATH   = r'C:\\Users\\juans\\Downloads\\dev_prev\\FORMATO HOJA DE CONTROL DOCUMENTAL.xlsx'\n",
    "OUTPUT_DIR_CTRL = r'C:\\Users\\juans\\Documents\\proarchitecg\\version_2_docker\\model_clasification_image_v2\\answer\\hojas_control'\n",
    "\n",
    "CONF_THRESH = 0.5\n",
    "SCORING_MIN = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c6218b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def find_persona_images(root):\n",
    "    persona_images = {}\n",
    "    for dirpath, _, files in os.walk(root):\n",
    "        imgs = [os.path.join(dirpath,f)\n",
    "                for f in files\n",
    "                if f.lower().endswith(('.png','.jpg','.jpeg'))]\n",
    "        if imgs:\n",
    "            pname = normalize_text(os.path.basename(dirpath))\n",
    "            persona_images.setdefault(pname, []).extend(imgs)\n",
    "    return persona_images\n",
    "\n",
    "def build_ocr_map(root):\n",
    "    json_map = {}\n",
    "    for dirpath, _, files in os.walk(root):\n",
    "        for fn in files:\n",
    "            if fn.lower().endswith('.json'):\n",
    "                key = normalize_text(os.path.splitext(fn)[0])\n",
    "                json_map[key] = os.path.join(dirpath, fn)\n",
    "    return json_map\n",
    "\n",
    "from rapidfuzz import fuzz\n",
    "\n",
    "def match_json_for_persona(json_map, persona, thresh=70):\n",
    "    # 1) intento exacto\n",
    "    if persona in json_map:\n",
    "        return json_map[persona]\n",
    "    # 2) intento fuzzy\n",
    "    best_score, best_key = 0, None\n",
    "    for k in json_map:\n",
    "        s = fuzz.partial_ratio(persona, k)\n",
    "        if s > best_score:\n",
    "            best_score, best_key = s, k\n",
    "    if best_score >= thresh:\n",
    "        return json_map[best_key]\n",
    "    return None\n",
    "# ── AUXILIARES ───────────────────────────────────────────────────────────────\n",
    "\n",
    "def extract_page_number(fn: str) -> float:\n",
    "    m = re.search(r'pagina[_-]?(\\d+)', fn, re.IGNORECASE) or re.search(r'(\\d+)', fn)\n",
    "    return int(m.group(1)) if m else math.nan\n",
    "\n",
    "def compile_dict(raw_dict):\n",
    "    out = {}\n",
    "    for label, pats in raw_dict.items():\n",
    "        pats = pats if isinstance(pats, list) else [pats]\n",
    "        comp = []\n",
    "        for pat in pats:\n",
    "            comp.append(pat if isinstance(pat, re.Pattern)\n",
    "                        else re.compile(rf\"\\b{pat}\\b\", re.IGNORECASE|re.VERBOSE))\n",
    "        out[label] = comp\n",
    "    return out\n",
    "\n",
    "def normalize_text(txt: str) -> str:\n",
    "    txt = unicodedata.normalize(\"NFKD\", txt).encode(\"ASCII\",\"ignore\").decode()\n",
    "    return re.sub(r'\\s+',' ', txt).strip().lower()\n",
    "\n",
    "def fuzzy_ocr_label(txt: str, label_patterns: dict, threshold: int=75):\n",
    "    best_label, best_score = \"\", 0.0\n",
    "    for label, patterns in label_patterns.items():\n",
    "        for pat in patterns:\n",
    "            s = fuzz.partial_ratio(txt, pat)\n",
    "            if s > best_score:\n",
    "                best_label, best_score = label, s\n",
    "    if best_score >= threshold:\n",
    "        return best_label, best_score/100.0\n",
    "    return \"\", 0.0\n",
    "\n",
    "def visual_predict(model, img_path, strict=True):\n",
    "    res = model.predict(source=img_path, device='cpu', task='classify', verbose=False)[0]\n",
    "    probs = getattr(res,'probs',None)\n",
    "    arr = probs.data.tolist() if hasattr(probs,'data') else list(probs or [])\n",
    "    if not arr or (strict and max(arr)<CONF_THRESH):\n",
    "        return None, (max(arr) if arr else 0.0)\n",
    "    idx = arr.index(max(arr))\n",
    "    return model.names[idx], max(arr)\n",
    "\n",
    "OCR_REGEX = compile_dict(OCR_PATTERNS)\n",
    "\n",
    "def classify(text, model, img_path):\n",
    "    txt = normalize_text(text or \"\")\n",
    "    # 1) Visual estricto\n",
    "    lbl, cf = visual_predict(model, img_path, strict=True)\n",
    "    if lbl: return lbl, cf, \"visual\"\n",
    "    # 2) OCR scoring dinámico\n",
    "    scores = {dt:0 for dt in OCR_REGEX}\n",
    "    for dt, pats in OCR_REGEX.items():\n",
    "        for p in pats:\n",
    "            if p.search(txt): scores[dt]+=1\n",
    "    th = {dt:max(1,len(OCR_REGEX[dt])//2) for dt in OCR_REGEX}\n",
    "    best_dt, cnt = max(scores.items(), key=lambda x:x[1])\n",
    "    if cnt>=th[best_dt]:\n",
    "        return best_dt, cnt/len(OCR_REGEX[best_dt]), \"ocr_scoring\"\n",
    "    #3) OCR regex rápido\n",
    "    for dt,pats in OCR_REGEX.items():\n",
    "        if any(p.search(txt) for p in pats):\n",
    "            return dt,1.0,\"ocr_regex\"\n",
    "    #4) fuzzy\n",
    "    simple = {dt:[p.pattern for p in OCR_REGEX[dt]] for dt in OCR_REGEX}\n",
    "    lbl_f, cf_f = fuzzy_ocr_label(txt, simple)\n",
    "    if lbl_f: return lbl_f, cf_f, \"ocr_fuzzy\"\n",
    "    return \"\",0.0,\"none\"\n",
    "\n",
    "def copy_row_format(ws, src_row: int, tgt_row: int, max_col: int = 13, row_height: float = 48):\n",
    "    ws.row_dimensions[tgt_row].height = row_height\n",
    "    thin = Side(border_style=\"thin\", color=\"000000\")\n",
    "    full_border = Border(left=thin, right=thin, top=thin, bottom=thin)\n",
    "\n",
    "    for col in range(1, max_col+1):\n",
    "        src = ws.cell(row=src_row, column=col)\n",
    "        tgt = ws.cell(row=tgt_row, column=col)\n",
    "        if src.has_style:\n",
    "            tgt.font           = copy(src.font)\n",
    "            tgt.fill           = copy(src.fill)\n",
    "            tgt.number_format  = copy(src.number_format)\n",
    "            tgt.protection     = copy(src.protection)\n",
    "            tgt.alignment      = copy(src.alignment)\n",
    "        tgt.border = full_border\n",
    "\n",
    "    for m in list(ws.merged_cells.ranges):\n",
    "        if m.min_row == src_row == m.max_row:\n",
    "            c1 = get_column_letter(m.min_col)\n",
    "            c2 = get_column_letter(m.max_col)\n",
    "            ws.merge_cells(f\"{c1}{tgt_row}:{c2}{tgt_row}\")\n",
    "\n",
    "def remove_holes(ws, hole_ranges):\n",
    "    \"\"\"\n",
    "    Descombina y borra TODO en los rangos de hole_ranges\n",
    "    (p.ej. [\"B57:B59\",\"C57:F59\",\"D60:F60\"]).\n",
    "    \"\"\"\n",
    "    # 1) parseamos rangos para detectar merges que intersecten\n",
    "    parsed = []\n",
    "    for rng in hole_ranges:\n",
    "        min_col, min_row, max_col, max_row = range_boundaries(rng)\n",
    "        parsed.append((min_row, max_row, min_col, max_col))\n",
    "\n",
    "    # 2) descombinamos merges que toquen esas zonas\n",
    "    to_unmerge = []\n",
    "    for m in list(ws.merged_cells.ranges):\n",
    "        for min_row, max_row, min_col, max_col in parsed:\n",
    "            if not (m.max_row < min_row or m.min_row > max_row\n",
    "                    or m.max_col < min_col or m.min_col > max_col):\n",
    "                to_unmerge.append(m.coord)\n",
    "                break\n",
    "    for coord in to_unmerge:\n",
    "        ws.unmerge_cells(coord)\n",
    "\n",
    "    # 3) borramos contenido de esas celdas\n",
    "    for rng in hole_ranges:\n",
    "        for row in ws[rng]:\n",
    "            for cell in row:\n",
    "                cell.value = None\n",
    "\n",
    "\n",
    "def generate_control_sheet(df_perso, persona):\n",
    "    out = os.path.join(OUTPUT_DIR_CTRL, f\"{persona}_hoja_control.xlsx\")\n",
    "    if os.path.exists(out):\n",
    "        print(f\"⚠️ Ya existe hoja de control para '{persona}', omitiendo.\")\n",
    "        return\n",
    "\n",
    "    wb = load_workbook(TEMPLATE_PATH)\n",
    "    ws = wb.active\n",
    "    START_ROW = 18\n",
    "\n",
    "    # 0) quitar huecos específicos antes de nada\n",
    "    holes = [\"B57:B59\", \"C57:F59\", \"D60:F60\"]\n",
    "    remove_holes(ws, holes)\n",
    "\n",
    "    # 1) localizar pie “NOMBRE Y APELLIDOS”\n",
    "    footer = None\n",
    "    for row in ws.iter_rows(min_row=START_ROW, max_row=ws.max_row):\n",
    "        for c in row:\n",
    "            if isinstance(c.value, str) and \"NOMBRE Y APELLIDOS\" in c.value.upper():\n",
    "                footer = c.row\n",
    "                break\n",
    "        if footer:\n",
    "            break\n",
    "    footer = footer or (START_ROW + 38)\n",
    "\n",
    "    # 2) detectar última fila con dato real en col A\n",
    "    content_rows = [\n",
    "        r for r in range(START_ROW, footer)\n",
    "        if ws.cell(row=r, column=1).value not in (None, \"\")\n",
    "    ]\n",
    "    if not content_rows:\n",
    "        raise RuntimeError(\"No encontré filas con contenido en la plantilla.\")\n",
    "    last_content = content_rows[-1]\n",
    "\n",
    "    # 3) insertar filas extras copiando formato desde last_content\n",
    "    template_n = len(content_rows)\n",
    "    n_pages    = len(df_perso)\n",
    "    if n_pages > template_n:\n",
    "        extras = n_pages - template_n\n",
    "        ws.insert_rows(footer, amount=extras)\n",
    "        for i in range(extras):\n",
    "            dst = footer + i\n",
    "            copy_row_format(ws, last_content, dst, max_col=13, row_height=48)\n",
    "\n",
    "    # 4) volcar datos de forma continua\n",
    "    for idx, rec in enumerate(df_perso.sort_values('posicion').itertuples(), start=1):\n",
    "        r = START_ROW + idx - 1\n",
    "        ws.cell(row=r, column=1, value=idx)               # A\n",
    "        ws.cell(row=r, column=5, value=rec.predicted)     # E\n",
    "        ws.cell(row=r, column=6, value=int(rec.posicion)) # F\n",
    "        ws.cell(row=r, column=7, value=int(rec.posicion)) # G\n",
    "\n",
    "    wb.save(out)\n",
    "    print(\"✅ Control inmediato:\", out)\n",
    "# ── MAIN ─────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def main():\n",
    "    os.makedirs(OUTPUT_DIR_CTRL, exist_ok=True)\n",
    "    model     = YOLO(MODEL_PATH)\n",
    "    OCR_REGEX = compile_dict(OCR_PATTERNS)\n",
    "\n",
    "    json_map       = build_ocr_map(OCR_ROOT)\n",
    "    persona_images = find_persona_images(IMAGES_ROOT)\n",
    "\n",
    "    all_rows, gid = [], 1\n",
    "\n",
    "    for pkey, img_paths in persona_images.items():\n",
    "        print(f\"\\nProcesando persona '{pkey}' con {len(img_paths)} imágenes…\")\n",
    "        texts        = {}\n",
    "        persona_rows = []  # <-- recolecta solo de esta persona\n",
    "\n",
    "        # cargar JSON de OCR si existe\n",
    "        jpath = match_json_for_persona(json_map, pkey)\n",
    "        if jpath:\n",
    "            with open(jpath, encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            recs = data if isinstance(data, list) else [data]\n",
    "            for rec in recs:\n",
    "                pg   = rec.get(\"pagina\")\n",
    "                imgf = rec.get(\"imagen\", \"\")\n",
    "                txt  = rec.get(\"texto\", \"\")\n",
    "                if pg   is not None:        texts[pg]               = txt\n",
    "                if imgf:                    texts[os.path.basename(imgf)] = txt\n",
    "\n",
    "        # clasificar imágenes\n",
    "        for img in sorted(img_paths, key=extract_page_number):\n",
    "            pg  = extract_page_number(img)\n",
    "            txt = texts.get(os.path.basename(img)) or texts.get(pg, \"\")\n",
    "            lbl, sc, ly = classify(txt, model, img)\n",
    "\n",
    "            rec = {\n",
    "                'id':        gid,\n",
    "                'persona':   pkey,\n",
    "                'imagen':    img,\n",
    "                'posicion':  pg,\n",
    "                'predicted': lbl,\n",
    "                'score':     sc,\n",
    "                'layer':     ly\n",
    "            }\n",
    "            all_rows.append(rec)\n",
    "            persona_rows.append(rec)\n",
    "            gid += 1\n",
    "\n",
    "        # en cuanto termino con esta persona, genero su hoja de control\n",
    "        if persona_rows:\n",
    "            df_perso = pd.DataFrame(persona_rows)\n",
    "            df_perso['correct'] = df_perso['persona'] == df_perso['predicted']\n",
    "            generate_control_sheet(df_perso, pkey)\n",
    "\n",
    "    # ─── Exportación y hojas de control ────────────────────────────\n",
    "    print(\"\\n⏳ Generando DataFrame y guardando\", OUTPUT_FILE)\n",
    "    df = pd.DataFrame(all_rows)\n",
    "    df['correct'] = df['persona'] == df['predicted']\n",
    "    df.to_excel(OUTPUT_FILE, index=False)\n",
    "    print(\"✅ Consolidado en\", OUTPUT_FILE)\n",
    "\n",
    "    # 4) Generar hojas de control dinámicas\n",
    "    START_ROW = 18\n",
    "    for persona, grp in df[df['predicted']!=\"\"].groupby('persona'):\n",
    "        wb = load_workbook(TEMPLATE_PATH)\n",
    "        ws = wb.active\n",
    "\n",
    "        # 4.1) hallar fila del pie buscando “NOMBRE Y APELLIDOS”\n",
    "        footer = None\n",
    "        for row in ws.iter_rows(min_row=START_ROW, max_row=ws.max_row):\n",
    "            for c in row:\n",
    "                if isinstance(c.value, str) and \"NOMBRE Y APELLIDOS\" in c.value.upper():\n",
    "                    footer = c.row\n",
    "                    break\n",
    "            if footer:\n",
    "                break\n",
    "        if not footer:\n",
    "            footer = START_ROW + 38  # fallback fijo\n",
    "\n",
    "        template_n = footer - START_ROW\n",
    "        n_pages    = len(grp)\n",
    "        # si faltan filas en la plantilla, las insertamos copiando formato\n",
    "        if n_pages > template_n:\n",
    "            extra = n_pages - template_n\n",
    "            ws.insert_rows(footer, amount=extra)\n",
    "            src = footer - 1\n",
    "            for i in range(extra):\n",
    "                dst = footer + i\n",
    "                ws.row_dimensions[dst].height = ws.row_dimensions[src].height\n",
    "                for col in (5,6,7):\n",
    "                    s = ws.cell(row=src, column=col)\n",
    "                    d = ws.cell(row=dst, column=col)\n",
    "                    d.font          = copy(s.font)\n",
    "                    d.border        = copy(s.border)\n",
    "                    d.fill          = copy(s.fill)\n",
    "                    d.alignment     = copy(s.alignment)\n",
    "                    d.number_format = s.number_format\n",
    "\n",
    "        # 4.2) escribir una fila por cada página\n",
    "        for idx, rec in enumerate(grp.sort_values('posicion').itertuples()):\n",
    "            r = START_ROW + idx\n",
    "            ws.cell(row=r, column=5, value=rec.predicted)\n",
    "            ws.cell(row=r, column=6, value=int(rec.posicion))\n",
    "            ws.cell(row=r, column=7, value=int(rec.posicion))\n",
    "\n",
    "        out = os.path.join(OUTPUT_DIR_CTRL, f\"{persona}_hoja_control.xlsx\")\n",
    "        wb.save(out)\n",
    "        print(\"✅ Control:\", out)\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
