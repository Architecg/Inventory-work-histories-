{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51fe5c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import json\n",
    "import unicodedata\n",
    "from typing import Dict, List, Tuple, Union\n",
    "\n",
    "import pandas as pd\n",
    "from copy import copy\n",
    "from rapidfuzz import fuzz\n",
    "from ultralytics import YOLO\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.utils import get_column_letter, range_boundaries\n",
    "from openpyxl.styles.borders import Border, Side\n",
    "\n",
    "from express import OCR_PATTERN as OCR_PATTERNS\n",
    "\n",
    "\n",
    "MODEL_PATH      = r'C:\\Users\\juans\\Documents\\proarchitecg\\version_2_docker\\model_clasification_image_v2\\runs\\classify\\person_cls\\weights\\best.pt'\n",
    "IMAGES_ROOT     = r'D:\\historias\\dev\\imagenes_por_doc\\LETRA B\\ACTA N° 71\\1'\n",
    "OCR_ROOT        = r'D:\\historias\\dev\\ocr_por_doc\\LETRA B\\ACTA N° 71\\1'\n",
    "OUTPUT_FILE     = r'C:\\Users\\juans\\Documents\\proarchitecg\\version_2_docker\\resultados_completos_v_final.xlsx'\n",
    "TEMPLATE_PATH   = r'C:\\Users\\juans\\Downloads\\dev_prev\\FORMATO HOJA DE CONTROL DOCUMENTAL.xlsx'\n",
    "OUTPUT_DIR_CTRL = r'C:\\Users\\juans\\Downloads\\define\\answer'\n",
    "\n",
    "CONF_THRESH = 0.5\n",
    "SCORING_MIN = 2  # (no se usa en la versión actual; mantenido por compatibilidad)\n",
    "\n",
    "# Diccionarios globales para patrones (se llenan en main)\n",
    "OCR_COMPILED: Dict[str, List[re.Pattern]] = {}\n",
    "OCR_SEEDS:    Dict[str, List[str]]        = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3d1b941",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(txt: str) -> str:\n",
    "    \"\"\"\n",
    "    Normaliza texto para matching:\n",
    "    - NFKD y elimina diacríticos\n",
    "    - Solo deja [a-z0-9 y espacio]\n",
    "    - Colapsa espacios y pasa a minúsculas\n",
    "    \"\"\"\n",
    "    if not isinstance(txt, str):\n",
    "        txt = \"\" if txt is None else str(txt)\n",
    "    nk = unicodedata.normalize(\"NFKD\", txt)\n",
    "    no_diac = \"\".join(ch for ch in nk if not unicodedata.combining(ch))\n",
    "    ascii_txt = no_diac.encode(\"ASCII\", \"ignore\").decode().lower()\n",
    "    ascii_txt = re.sub(r\"[^a-z0-9\\s]\", \" \", ascii_txt)\n",
    "    ascii_txt = re.sub(r\"\\s+\", \" \", ascii_txt).strip()\n",
    "    return ascii_txt\n",
    "\n",
    "\n",
    "def regex_to_seed(pat: Union[str, re.Pattern]) -> str:\n",
    "    \"\"\"\n",
    "    Extrae una 'semilla' legible para fuzzy desde un patrón:\n",
    "    - Si es re.Pattern toma .pattern\n",
    "    - Elimina metacaracteres y normaliza\n",
    "    \"\"\"\n",
    "    raw = pat.pattern if isinstance(pat, re.Pattern) else str(pat)\n",
    "    raw = raw.replace(r\"\\b\", \" \")\n",
    "    raw = re.sub(r\"[^a-zA-Z0-9\\s]\", \" \", raw)\n",
    "    raw = re.sub(r\"\\s+\", \" \", raw).strip()\n",
    "    return normalize_text(raw)\n",
    "\n",
    "\n",
    "def prepare_patterns(raw_dict: Dict[str, Union[str, re.Pattern, List[Union[str, re.Pattern]]]]\n",
    "                    ) -> Tuple[Dict[str, List[re.Pattern]], Dict[str, List[str]]]:\n",
    "    \"\"\"\n",
    "    Acepta {label: [patrones]} como strings (normalizados) o re.Patterns.\n",
    "    Devuelve:\n",
    "      - compiled[label] -> list[re.Pattern] (sobre texto ya normalizado)\n",
    "      - seeds[label]    -> list[str] para fuzzy\n",
    "    \"\"\"\n",
    "    compiled: Dict[str, List[re.Pattern]] = {}\n",
    "    seeds:    Dict[str, List[str]]        = {}\n",
    "\n",
    "    for label, pats in raw_dict.items():\n",
    "        arr = pats if isinstance(pats, list) else [pats]\n",
    "        comp_list: List[re.Pattern] = []\n",
    "        seed_list: List[str] = []\n",
    "\n",
    "        for pat in arr:\n",
    "            if isinstance(pat, re.Pattern):\n",
    "                comp_list.append(pat)\n",
    "                seed_list.append(regex_to_seed(pat))\n",
    "            else:\n",
    "                literal = normalize_text(pat)\n",
    "                if not literal:\n",
    "                    continue\n",
    "                seed_list.append(literal)\n",
    "                comp_list.append(re.compile(rf\"\\b{re.escape(literal)}\\b\"))\n",
    "\n",
    "        if comp_list:\n",
    "            compiled[label] = comp_list\n",
    "            seeds[label]    = [s for s in seed_list if s]\n",
    "\n",
    "    return compiled, seeds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86995c42",
   "metadata": {},
   "source": [
    "# helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f62cc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # 3) Helpers de IO / OCR\n",
    "\n",
    "# %%\n",
    "def normalize_basename(path: str) -> str:\n",
    "    base = os.path.basename(path)\n",
    "    stem, _ = os.path.splitext(base)\n",
    "    return normalize_text(stem)\n",
    "\n",
    "\n",
    "def load_ocr_records(jpath: str):\n",
    "    \"\"\"\n",
    "    Carga un OCR JSON y devuelve lista de registros con posible forma:\n",
    "      - [ {pagina, imagen, texto}, ... ]\n",
    "      - { \"paginas\"/\"pages\"/\"data\"/\"items\"/\"ocr\": [ {...}, ... ] }\n",
    "      - JSON Lines (una por línea)\n",
    "      - Un único objeto {pagina, imagen, texto}\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(jpath, encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "    except json.JSONDecodeError:\n",
    "        # NDJSON / JSONL\n",
    "        recs = []\n",
    "        with open(jpath, encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                try:\n",
    "                    recs.append(json.loads(line))\n",
    "                except json.JSONDecodeError:\n",
    "                    pass\n",
    "        return recs\n",
    "\n",
    "    if isinstance(data, list):\n",
    "        return data\n",
    "    if isinstance(data, dict):\n",
    "        for key in (\"paginas\", \"pages\", \"data\", \"items\", \"ocr\"):\n",
    "            if key in data and isinstance(data[key], list):\n",
    "                return data[key]\n",
    "        return [data]\n",
    "    return []\n",
    "\n",
    "\n",
    "def map_ocr_to_indexes(recs, texts_by_page: Dict[int, str], texts_by_file: Dict[str, str]):\n",
    "    \"\"\"\n",
    "    Agrega registros OCR a dos índices:\n",
    "      - texts_by_page[int] = texto\n",
    "      - texts_by_file[nombre_archivo_normalizado] = texto\n",
    "    \"\"\"\n",
    "    added_p, added_f = 0, 0\n",
    "    for rec in recs:\n",
    "        if not isinstance(rec, dict):\n",
    "            continue\n",
    "\n",
    "        txt = rec.get(\"texto\", \"\") or rec.get(\"text\", \"\") or \"\"\n",
    "\n",
    "        # (1) mapear por número de página\n",
    "        pg_int = None\n",
    "        if \"pagina\" in rec:\n",
    "            try:\n",
    "                pg_int = int(str(rec[\"pagina\"]).strip())\n",
    "            except Exception:\n",
    "                pg_int = None\n",
    "        elif \"page\" in rec:\n",
    "            try:\n",
    "                pg_int = int(str(rec[\"page\"]).strip())\n",
    "            except Exception:\n",
    "                pg_int = None\n",
    "        if pg_int is not None:\n",
    "            texts_by_page[pg_int] = txt\n",
    "            added_p += 1\n",
    "\n",
    "        # (2) mapear por nombre de archivo normalizado\n",
    "        imgf = rec.get(\"imagen\") or rec.get(\"image\") or \"\"\n",
    "        if imgf:\n",
    "            texts_by_file[normalize_basename(imgf)] = txt\n",
    "            added_f += 1\n",
    "            # inferir página del nombre\n",
    "            pg_from_img = extract_page_number(os.path.basename(imgf))\n",
    "            if isinstance(pg_from_img, (int, float)) and not math.isnan(pg_from_img):\n",
    "                texts_by_page[int(pg_from_img)] = txt\n",
    "                added_p += 1\n",
    "\n",
    "    return added_p, added_f\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab7d39b",
   "metadata": {},
   "source": [
    "# Busqueda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07c57cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # 4) Búsqueda de imágenes/JSON y helpers\n",
    "\n",
    "# %%\n",
    "def find_persona_images(root: str) -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Recorre la carpeta raíz y agrupa imágenes por persona (nombre de carpeta base).\n",
    "    \"\"\"\n",
    "    persona_images: Dict[str, List[str]] = {}\n",
    "    for dirpath, _, files in os.walk(root):\n",
    "        imgs = [os.path.join(dirpath, f)\n",
    "                for f in files\n",
    "                if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        if imgs:\n",
    "            pname = normalize_text(os.path.basename(dirpath))\n",
    "            persona_images.setdefault(pname, []).extend(imgs)\n",
    "    return persona_images\n",
    "\n",
    "\n",
    "def build_ocr_map(root: str) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Indexa archivos .json (OCR) por nombre de archivo (sin extensión) normalizado.\n",
    "    \"\"\"\n",
    "    json_map: Dict[str, str] = {}\n",
    "    for dirpath, _, files in os.walk(root):\n",
    "        for fn in files:\n",
    "            if fn.lower().endswith('.json'):\n",
    "                key = normalize_text(os.path.splitext(fn)[0])\n",
    "                json_map[key] = os.path.join(dirpath, fn)\n",
    "    return json_map\n",
    "\n",
    "\n",
    "def match_json_for_persona(json_map: Dict[str, str], persona: str, thresh: int = 70):\n",
    "    \"\"\"\n",
    "    Devuelve la ruta del JSON cuyo nombre más se parece a 'persona' (exacto o fuzzy).\n",
    "    \"\"\"\n",
    "    if persona in json_map:\n",
    "        return json_map[persona]\n",
    "    best_score, best_key = 0, None\n",
    "    for k in json_map:\n",
    "        s = fuzz.partial_ratio(persona, k)\n",
    "        if s > best_score:\n",
    "            best_score, best_key = s, k\n",
    "    if best_score >= thresh and best_key:\n",
    "        return json_map[best_key]\n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_page_number(fn: str) -> float:\n",
    "    \"\"\"\n",
    "    Extrae un número de página desde 'pagina_12.png' u otro entero en el nombre.\n",
    "    \"\"\"\n",
    "    m = re.search(r'pagina[_-]?(\\d+)', fn, re.IGNORECASE) or re.search(r'(\\d+)', fn)\n",
    "    return int(m.group(1)) if m else math.nan\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# # 5) Predicción visual, fuzzy y clasificador\n",
    "\n",
    "# %%\n",
    "def visual_predict(model: YOLO, img_path: str, strict: bool = True):\n",
    "    \"\"\"\n",
    "    Predicción visual (clasificador Ultralytics).\n",
    "    \"\"\"\n",
    "    res = model.predict(source=img_path, device='cpu', task='classify', verbose=False)[0]\n",
    "    probs = getattr(res, 'probs', None)\n",
    "    arr = probs.data.tolist() if hasattr(probs, 'data') else list(probs or [])\n",
    "    if not arr or (strict and max(arr) < CONF_THRESH):\n",
    "        return None, (max(arr) if arr else 0.0)\n",
    "    idx = arr.index(max(arr))\n",
    "    return model.names[idx], max(arr)\n",
    "\n",
    "\n",
    "def fuzzy_ocr_label(txt_norm: str,\n",
    "                    label_seeds: Dict[str, List[str]],\n",
    "                    threshold: int = 75) -> Tuple[str, float]:\n",
    "    \"\"\"\n",
    "    Fuzzy con semillas 'planas' (sin regex). Usa partial_ratio.\n",
    "    \"\"\"\n",
    "    best_label, best_score = \"\", 0\n",
    "    for label, terms in label_seeds.items():\n",
    "        for t in terms:\n",
    "            if not t:\n",
    "                continue\n",
    "            s = fuzz.partial_ratio(txt_norm, t)\n",
    "            if s > best_score:\n",
    "                best_label, best_score = label, s\n",
    "    return (best_label, best_score / 100.0) if best_score >= threshold else (\"\", 0.0)\n",
    "\n",
    "\n",
    "def classify(text: str, model: YOLO, img_path: str):\n",
    "    \"\"\"\n",
    "    Pipeline de clasificación:\n",
    "      1) Visual estricto\n",
    "      2) OCR scoring (cuenta matches por label)\n",
    "      3) OCR regex rápido (primer match)\n",
    "      4) Fuzzy (semillas planas)\n",
    "    \"\"\"\n",
    "    txt = normalize_text(text or \"\")\n",
    "\n",
    "    # 1) Visual\n",
    "    lbl, cf = visual_predict(model, img_path, strict=True)\n",
    "    if lbl:\n",
    "        return lbl, cf, \"visual\"\n",
    "\n",
    "    # 2) OCR scoring\n",
    "    scores = {dt: 0 for dt in OCR_COMPILED}\n",
    "    for dt, pats in OCR_COMPILED.items():\n",
    "        for p in pats:\n",
    "            if p.search(txt):\n",
    "                scores[dt] += 1\n",
    "    if scores:\n",
    "        th = {dt: max(1, len(OCR_COMPILED[dt]) // 2) for dt in OCR_COMPILED}\n",
    "        best_dt, cnt = max(scores.items(), key=lambda x: x[1])\n",
    "        if cnt >= th[best_dt]:\n",
    "            conf = cnt / max(1, len(OCR_COMPILED[best_dt]))\n",
    "            return best_dt, conf, \"ocr_scoring\"\n",
    "\n",
    "    # 3) OCR regex rápido\n",
    "    for dt, pats in OCR_COMPILED.items():\n",
    "        for p in pats:\n",
    "            if p.search(txt):\n",
    "                return dt, 1.0, \"ocr_regex\"\n",
    "\n",
    "    # 4) Fuzzy\n",
    "    lbl_f, cf_f = fuzzy_ocr_label(txt, OCR_SEEDS, threshold=75)\n",
    "    if lbl_f:\n",
    "        return lbl_f, cf_f, \"ocr_fuzzy\"\n",
    "\n",
    "    return \"\", 0.0, \"none\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21529fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # 6) Utilidades de Excel\n",
    "\n",
    "# %%\n",
    "def copy_row_format(ws, src_row: int, tgt_row: int, max_col: int = 13, row_height: float = 48):\n",
    "    ws.row_dimensions[tgt_row].height = row_height\n",
    "    thin = Side(border_style=\"thin\", color=\"000000\")\n",
    "    full_border = Border(left=thin, right=thin, top=thin, bottom=thin)\n",
    "\n",
    "    for col in range(1, max_col + 1):\n",
    "        src = ws.cell(row=src_row, column=col)\n",
    "        tgt = ws.cell(row=tgt_row, column=col)\n",
    "        if src.has_style:\n",
    "            tgt.font          = copy(src.font)\n",
    "            tgt.fill          = copy(src.fill)\n",
    "            tgt.number_format = copy(src.number_format)\n",
    "            tgt.protection    = copy(src.protection)\n",
    "            tgt.alignment     = copy(src.alignment)\n",
    "        tgt.border = full_border\n",
    "\n",
    "    for m in list(ws.merged_cells.ranges):\n",
    "        if m.min_row == src_row == m.max_row:\n",
    "            c1 = get_column_letter(m.min_col)\n",
    "            c2 = get_column_letter(m.max_col)\n",
    "            ws.merge_cells(f\"{c1}{tgt_row}:{c2}{tgt_row}\")\n",
    "\n",
    "\n",
    "def remove_holes(ws, hole_ranges: List[str]):\n",
    "    \"\"\"\n",
    "    Descombina y borra TODO en los rangos indicados (p.ej. [\"B57:B59\",\"C57:F59\",\"D60:F60\"])\n",
    "    \"\"\"\n",
    "    parsed = []\n",
    "    for rng in hole_ranges:\n",
    "        min_col, min_row, max_col, max_row = range_boundaries(rng)\n",
    "        parsed.append((min_row, max_row, min_col, max_col))\n",
    "\n",
    "    to_unmerge = []\n",
    "    for m in list(ws.merged_cells.ranges):\n",
    "        for min_row, max_row, min_col, max_col in parsed:\n",
    "            if not (m.max_row < min_row or m.min_row > max_row\n",
    "                    or m.max_col < min_col or m.min_col > max_col):\n",
    "                to_unmerge.append(m.coord)\n",
    "                break\n",
    "    for coord in to_unmerge:\n",
    "        ws.unmerge_cells(coord)\n",
    "\n",
    "    for rng in hole_ranges:\n",
    "        for row in ws[rng]:\n",
    "            for cell in row:\n",
    "                cell.value = None\n",
    "\n",
    "\n",
    "def generate_control_sheet(df_perso: pd.DataFrame, persona: str):\n",
    "    \"\"\"\n",
    "    Genera y guarda la hoja de control para una persona (si no existe).\n",
    "    \"\"\"\n",
    "    os.makedirs(OUTPUT_DIR_CTRL, exist_ok=True)\n",
    "    out = os.path.join(OUTPUT_DIR_CTRL, f\"{persona}_hoja_control.xlsx\")\n",
    "    if os.path.exists(out):\n",
    "        print(f\"⚠️ Ya existe hoja de control para '{persona}', omitiendo.\")\n",
    "        return\n",
    "\n",
    "    wb = load_workbook(TEMPLATE_PATH)\n",
    "    ws = wb.active\n",
    "    START_ROW = 18\n",
    "\n",
    "    # Quitar huecos específicos\n",
    "    holes = [\"B57:B59\", \"C57:F59\", \"D60:F60\"]\n",
    "    remove_holes(ws, holes)\n",
    "\n",
    "    # Buscar pie “NOMBRE Y APELLIDOS”\n",
    "    footer = None\n",
    "    for row in ws.iter_rows(min_row=START_ROW, max_row=ws.max_row):\n",
    "        for c in row:\n",
    "            if isinstance(c.value, str) and \"NOMBRE Y APELLIDOS\" in c.value.upper():\n",
    "                footer = c.row\n",
    "                break\n",
    "        if footer:\n",
    "            break\n",
    "    footer = footer or (START_ROW + 38)\n",
    "\n",
    "    # Detectar filas con contenido en A\n",
    "    content_rows = [\n",
    "        r for r in range(START_ROW, footer)\n",
    "        if ws.cell(row=r, column=1).value not in (None, \"\")\n",
    "    ]\n",
    "    if not content_rows:\n",
    "        raise RuntimeError(\"No encontré filas con contenido en la plantilla.\")\n",
    "    last_content = content_rows[-1]\n",
    "\n",
    "    # Insertar filas necesarias\n",
    "    template_n = len(content_rows)\n",
    "    n_pages    = len(df_perso)\n",
    "    if n_pages > template_n:\n",
    "        extras = n_pages - template_n\n",
    "        ws.insert_rows(footer, amount=extras)\n",
    "        for i in range(extras):\n",
    "            dst = footer + i\n",
    "            copy_row_format(ws, last_content, dst, max_col=13, row_height=48)\n",
    "\n",
    "    # Volcar datos\n",
    "    for idx, rec in enumerate(df_perso.sort_values('posicion').itertuples(), start=1):\n",
    "        r = START_ROW + idx - 1\n",
    "        ws.cell(row=r, column=1, value=idx)               # A\n",
    "        ws.cell(row=r, column=5, value=rec.predicted)     # E\n",
    "        ws.cell(row=r, column=6, value=int(rec.posicion)) # F\n",
    "        ws.cell(row=r, column=7, value=int(rec.posicion)) # G\n",
    "\n",
    "    wb.save(out)\n",
    "    print(\"✅ Control inmediato:\", out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c801a16e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Cargando modelo YOLO...\n",
      "✅ Modelo cargado.\n",
      "⏳ Compilando patrones OCR...\n",
      "✅ Patrones compilados: 37 tipos.\n",
      "⏳ Indexando OCR JSON...\n",
      "✅ JSONs indexados: 9\n",
      "⏳ Buscando imágenes por persona...\n",
      "✅ Personas detectadas: 9\n",
      "\n",
      "👤 Procesando persona 'babilonia negrette harold' con 8 imágenes…\n",
      "   🔎 OCR JSON: Babilonia Negrette Harold.json | registros=8 | páginas=8 (+16) | archivos=8 (+8)\n",
      "⚠️ Ya existe hoja de control para 'babilonia negrette harold', omitiendo.\n",
      "\n",
      "👤 Procesando persona 'bacca castro diego armando' con 18 imágenes…\n",
      "   🔎 OCR JSON: Bacca Castro Diego Armando.json | registros=18 | páginas=18 (+36) | archivos=18 (+18)\n",
      "⚠️ Ya existe hoja de control para 'bacca castro diego armando', omitiendo.\n",
      "\n",
      "👤 Procesando persona 'bacca castro luis fernando' con 25 imágenes…\n",
      "   🔎 OCR JSON: Bacca Castro Luis Fernando.json | registros=25 | páginas=25 (+50) | archivos=25 (+25)\n",
      "⚠️ Ya existe hoja de control para 'bacca castro luis fernando', omitiendo.\n",
      "\n",
      "👤 Procesando persona 'bacca de valencia adelaida del carmen 1' con 221 imágenes…\n",
      "   🔎 OCR JSON: Bacca De Valencia Adelaida Del Carmen 1.json | registros=221 | páginas=221 (+442) | archivos=221 (+221)\n",
      "   ⚠️ OCR vacío para 'pagina_104.png' (pg=104). Usaré solo visual + fuzzy/regex sobre texto vacío.\n",
      "   ⚠️ OCR vacío para 'pagina_198.png' (pg=198). Usaré solo visual + fuzzy/regex sobre texto vacío.\n",
      "   ⚠️ OCR vacío para 'pagina_212.png' (pg=212). Usaré solo visual + fuzzy/regex sobre texto vacío.\n",
      "⚠️ Ya existe hoja de control para 'bacca de valencia adelaida del carmen 1', omitiendo.\n",
      "\n",
      "👤 Procesando persona 'bacca de valencia adelaida del carmen 2' con 223 imágenes…\n",
      "   🔎 OCR JSON: Bacca De Valencia Adelaida Del Carmen 2.json | registros=223 | páginas=223 (+446) | archivos=223 (+223)\n",
      "   ⚠️ OCR vacío para 'pagina_38.png' (pg=38). Usaré solo visual + fuzzy/regex sobre texto vacío.\n",
      "⚠️ Ya existe hoja de control para 'bacca de valencia adelaida del carmen 2', omitiendo.\n",
      "\n",
      "👤 Procesando persona 'bacca de valencia adelaida del carmen 3' con 209 imágenes…\n",
      "   🔎 OCR JSON: Bacca De Valencia Adelaida Del Carmen 3.json | registros=209 | páginas=209 (+418) | archivos=209 (+209)\n",
      "⚠️ Ya existe hoja de control para 'bacca de valencia adelaida del carmen 3', omitiendo.\n",
      "\n",
      "👤 Procesando persona 'bacca de valencia adelaida del carmen 4' con 132 imágenes…\n",
      "   🔎 OCR JSON: Bacca De Valencia Adelaida Del Carmen 4.json | registros=132 | páginas=132 (+264) | archivos=132 (+132)\n",
      "⚠️ Ya existe hoja de control para 'bacca de valencia adelaida del carmen 4', omitiendo.\n",
      "\n",
      "👤 Procesando persona 'bacca jorge heriberto' con 19 imágenes…\n",
      "   🔎 OCR JSON: Bacca Jorge Heriberto.json | registros=19 | páginas=19 (+38) | archivos=19 (+19)\n",
      "⚠️ Ya existe hoja de control para 'bacca jorge heriberto', omitiendo.\n",
      "\n",
      "👤 Procesando persona 'bacca soto carlos' con 11 imágenes…\n",
      "   🔎 OCR JSON: Bacca Soto Carlos.json | registros=11 | páginas=11 (+22) | archivos=11 (+11)\n",
      "⚠️ Ya existe hoja de control para 'bacca soto carlos', omitiendo.\n",
      "\n",
      "⏳ Generando DataFrame y guardando: C:\\Users\\juans\\Documents\\proarchitecg\\version_2_docker\\resultados_completos_v_final.xlsx\n",
      "✅ Consolidado en C:\\Users\\juans\\Documents\\proarchitecg\\version_2_docker\\resultados_completos_v_final.xlsx\n",
      "🎉 Proceso completado.\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # 7) Main y ejecución\n",
    "\n",
    "# %%\n",
    "def main():\n",
    "    global OCR_COMPILED, OCR_SEEDS  # para que classify() use los compilados\n",
    "\n",
    "    os.makedirs(OUTPUT_DIR_CTRL, exist_ok=True)\n",
    "\n",
    "    print(\"⏳ Cargando modelo YOLO...\")\n",
    "    model = YOLO(MODEL_PATH)\n",
    "    print(\"✅ Modelo cargado.\")\n",
    "\n",
    "    print(\"⏳ Compilando patrones OCR...\")\n",
    "    OCR_COMPILED, OCR_SEEDS = prepare_patterns(OCR_PATTERNS)\n",
    "    if not OCR_COMPILED:\n",
    "        raise RuntimeError(\"OCR_COMPILED vacío: revisa doc_type_patterns.OCR_PATTERNS\")\n",
    "    print(f\"✅ Patrones compilados: {len(OCR_COMPILED)} tipos.\")\n",
    "\n",
    "    # Indexación\n",
    "    print(\"⏳ Indexando OCR JSON...\")\n",
    "    json_map = build_ocr_map(OCR_ROOT)\n",
    "    print(f\"✅ JSONs indexados: {len(json_map)}\")\n",
    "\n",
    "    print(\"⏳ Buscando imágenes por persona...\")\n",
    "    persona_images = find_persona_images(IMAGES_ROOT)\n",
    "    print(f\"✅ Personas detectadas: {len(persona_images)}\")\n",
    "\n",
    "    all_rows, gid = [], 1\n",
    "\n",
    "    # Procesamiento por persona\n",
    "    for pkey, img_paths in persona_images.items():\n",
    "        print(f\"\\n👤 Procesando persona '{pkey}' con {len(img_paths)} imágenes…\")\n",
    "\n",
    "        # Índices para el OCR\n",
    "        texts_by_page: Dict[int, str] = {}\n",
    "        texts_by_file: Dict[str, str]  = {}\n",
    "\n",
    "        # Cargar y mapear OCR JSON\n",
    "        jpath = match_json_for_persona(json_map, pkey)\n",
    "        if jpath:\n",
    "            try:\n",
    "                recs = load_ocr_records(jpath)\n",
    "                added_p, added_f = map_ocr_to_indexes(recs, texts_by_page, texts_by_file)\n",
    "                print(f\"   🔎 OCR JSON: {os.path.basename(jpath)} | \"\n",
    "                      f\"registros={len(recs)} | páginas={len(texts_by_page)} (+{added_p}) | \"\n",
    "                      f\"archivos={len(texts_by_file)} (+{added_f})\")\n",
    "                if not texts_by_page and not texts_by_file and recs:\n",
    "                    sample = recs[0]\n",
    "                    print(f\"   ℹ️ Estructura ejemplo del JSON: keys={list(sample.keys())[:8]}\")\n",
    "            except Exception as e:\n",
    "                print(f\"   ⚠️ Error leyendo/mapeando JSON '{jpath}': {e}\")\n",
    "        else:\n",
    "            print(\"   ⚠️ No se encontró JSON OCR para esta persona (se continúa con visual/fuzzy/regex).\")\n",
    "\n",
    "        # Clasificar imágenes\n",
    "        persona_rows = []\n",
    "        for img in sorted(img_paths, key=extract_page_number):\n",
    "            pg_val = extract_page_number(img)\n",
    "            pg_int = int(pg_val) if isinstance(pg_val, (int, float)) and not math.isnan(pg_val) else None\n",
    "\n",
    "            key_norm = normalize_basename(img)\n",
    "            txt = texts_by_file.get(key_norm, \"\")\n",
    "            if not txt and pg_int is not None:\n",
    "                txt = texts_by_page.get(pg_int, \"\")\n",
    "\n",
    "            if not txt:\n",
    "                print(f\"   ⚠️ OCR vacío para '{os.path.basename(img)}' (pg={pg_int}). \"\n",
    "                      f\"Usaré solo visual + fuzzy/regex sobre texto vacío.\")\n",
    "\n",
    "            lbl, sc, ly = classify(txt, model, img)\n",
    "\n",
    "            rec = {\n",
    "                'id':        gid,\n",
    "                'persona':   pkey,\n",
    "                'imagen':    img,\n",
    "                'posicion':  pg_val,\n",
    "                'predicted': lbl,\n",
    "                'score':     sc,\n",
    "                'layer':     ly\n",
    "            }\n",
    "            all_rows.append(rec)\n",
    "            persona_rows.append(rec)\n",
    "            gid += 1\n",
    "\n",
    "        # Generar hoja de control por persona inmediatamente\n",
    "        if persona_rows:\n",
    "            df_perso = pd.DataFrame(persona_rows)\n",
    "            df_perso['correct'] = df_perso['persona'] == df_perso['predicted']  # compat\n",
    "            generate_control_sheet(df_perso, pkey)\n",
    "\n",
    "    # Consolidado global\n",
    "    print(\"\\n⏳ Generando DataFrame y guardando:\", OUTPUT_FILE)\n",
    "    df = pd.DataFrame(all_rows)\n",
    "    if not df.empty:\n",
    "        df['correct'] = df['persona'] == df['predicted']\n",
    "        df.to_excel(OUTPUT_FILE, index=False)\n",
    "        print(\"✅ Consolidado en\", OUTPUT_FILE)\n",
    "    else:\n",
    "        print(\"⚠️ No se generaron filas; revisa rutas y extensiones.\")\n",
    "\n",
    "    print(\"🎉 Proceso completado.\")\n",
    "\n",
    "\n",
    "# %%\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clasificador_docs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
