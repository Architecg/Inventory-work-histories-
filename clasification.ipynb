{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51fe5c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import json\n",
    "import unicodedata\n",
    "from typing import Dict, List, Tuple, Union\n",
    "\n",
    "import pandas as pd\n",
    "from copy import copy\n",
    "from rapidfuzz import fuzz\n",
    "from ultralytics import YOLO\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.utils import get_column_letter, range_boundaries\n",
    "from openpyxl.styles.borders import Border, Side\n",
    "\n",
    "from express import OCR_PATTERN as OCR_PATTERNS\n",
    "\n",
    "\n",
    "MODEL_PATH      = r'C:\\Users\\juans\\Documents\\proarchitecg\\version_2_docker\\model_clasification_image_v2\\runs\\classify\\person_cls\\weights\\best.pt'\n",
    "IMAGES_ROOT     = r'D:\\historias\\dev\\imagenes_por_doc\\LETRA B\\ACTA N¬∞ 71\\1'\n",
    "OCR_ROOT        = r'D:\\historias\\dev\\ocr_por_doc\\LETRA B\\ACTA N¬∞ 71\\1'\n",
    "OUTPUT_FILE     = r'C:\\Users\\juans\\Documents\\proarchitecg\\version_2_docker\\resultados_completos_v_final.xlsx'\n",
    "TEMPLATE_PATH   = r'C:\\Users\\juans\\Downloads\\dev_prev\\FORMATO HOJA DE CONTROL DOCUMENTAL.xlsx'\n",
    "OUTPUT_DIR_CTRL = r'C:\\Users\\juans\\Downloads\\define\\answer'\n",
    "\n",
    "CONF_THRESH = 0.5\n",
    "SCORING_MIN = 2  # (no se usa en la versi√≥n actual; mantenido por compatibilidad)\n",
    "\n",
    "# Diccionarios globales para patrones (se llenan en main)\n",
    "OCR_COMPILED: Dict[str, List[re.Pattern]] = {}\n",
    "OCR_SEEDS:    Dict[str, List[str]]        = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3d1b941",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(txt: str) -> str:\n",
    "    \"\"\"\n",
    "    Normaliza texto para matching:\n",
    "    - NFKD y elimina diacr√≠ticos\n",
    "    - Solo deja [a-z0-9 y espacio]\n",
    "    - Colapsa espacios y pasa a min√∫sculas\n",
    "    \"\"\"\n",
    "    if not isinstance(txt, str):\n",
    "        txt = \"\" if txt is None else str(txt)\n",
    "    nk = unicodedata.normalize(\"NFKD\", txt)\n",
    "    no_diac = \"\".join(ch for ch in nk if not unicodedata.combining(ch))\n",
    "    ascii_txt = no_diac.encode(\"ASCII\", \"ignore\").decode().lower()\n",
    "    ascii_txt = re.sub(r\"[^a-z0-9\\s]\", \" \", ascii_txt)\n",
    "    ascii_txt = re.sub(r\"\\s+\", \" \", ascii_txt).strip()\n",
    "    return ascii_txt\n",
    "\n",
    "\n",
    "def regex_to_seed(pat: Union[str, re.Pattern]) -> str:\n",
    "    \"\"\"\n",
    "    Extrae una 'semilla' legible para fuzzy desde un patr√≥n:\n",
    "    - Si es re.Pattern toma .pattern\n",
    "    - Elimina metacaracteres y normaliza\n",
    "    \"\"\"\n",
    "    raw = pat.pattern if isinstance(pat, re.Pattern) else str(pat)\n",
    "    raw = raw.replace(r\"\\b\", \" \")\n",
    "    raw = re.sub(r\"[^a-zA-Z0-9\\s]\", \" \", raw)\n",
    "    raw = re.sub(r\"\\s+\", \" \", raw).strip()\n",
    "    return normalize_text(raw)\n",
    "\n",
    "\n",
    "def prepare_patterns(raw_dict: Dict[str, Union[str, re.Pattern, List[Union[str, re.Pattern]]]]\n",
    "                    ) -> Tuple[Dict[str, List[re.Pattern]], Dict[str, List[str]]]:\n",
    "    \"\"\"\n",
    "    Acepta {label: [patrones]} como strings (normalizados) o re.Patterns.\n",
    "    Devuelve:\n",
    "      - compiled[label] -> list[re.Pattern] (sobre texto ya normalizado)\n",
    "      - seeds[label]    -> list[str] para fuzzy\n",
    "    \"\"\"\n",
    "    compiled: Dict[str, List[re.Pattern]] = {}\n",
    "    seeds:    Dict[str, List[str]]        = {}\n",
    "\n",
    "    for label, pats in raw_dict.items():\n",
    "        arr = pats if isinstance(pats, list) else [pats]\n",
    "        comp_list: List[re.Pattern] = []\n",
    "        seed_list: List[str] = []\n",
    "\n",
    "        for pat in arr:\n",
    "            if isinstance(pat, re.Pattern):\n",
    "                comp_list.append(pat)\n",
    "                seed_list.append(regex_to_seed(pat))\n",
    "            else:\n",
    "                literal = normalize_text(pat)\n",
    "                if not literal:\n",
    "                    continue\n",
    "                seed_list.append(literal)\n",
    "                comp_list.append(re.compile(rf\"\\b{re.escape(literal)}\\b\"))\n",
    "\n",
    "        if comp_list:\n",
    "            compiled[label] = comp_list\n",
    "            seeds[label]    = [s for s in seed_list if s]\n",
    "\n",
    "    return compiled, seeds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86995c42",
   "metadata": {},
   "source": [
    "# helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f62cc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # 3) Helpers de IO / OCR\n",
    "\n",
    "# %%\n",
    "def normalize_basename(path: str) -> str:\n",
    "    base = os.path.basename(path)\n",
    "    stem, _ = os.path.splitext(base)\n",
    "    return normalize_text(stem)\n",
    "\n",
    "\n",
    "def load_ocr_records(jpath: str):\n",
    "    \"\"\"\n",
    "    Carga un OCR JSON y devuelve lista de registros con posible forma:\n",
    "      - [ {pagina, imagen, texto}, ... ]\n",
    "      - { \"paginas\"/\"pages\"/\"data\"/\"items\"/\"ocr\": [ {...}, ... ] }\n",
    "      - JSON Lines (una por l√≠nea)\n",
    "      - Un √∫nico objeto {pagina, imagen, texto}\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(jpath, encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "    except json.JSONDecodeError:\n",
    "        # NDJSON / JSONL\n",
    "        recs = []\n",
    "        with open(jpath, encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                try:\n",
    "                    recs.append(json.loads(line))\n",
    "                except json.JSONDecodeError:\n",
    "                    pass\n",
    "        return recs\n",
    "\n",
    "    if isinstance(data, list):\n",
    "        return data\n",
    "    if isinstance(data, dict):\n",
    "        for key in (\"paginas\", \"pages\", \"data\", \"items\", \"ocr\"):\n",
    "            if key in data and isinstance(data[key], list):\n",
    "                return data[key]\n",
    "        return [data]\n",
    "    return []\n",
    "\n",
    "\n",
    "def map_ocr_to_indexes(recs, texts_by_page: Dict[int, str], texts_by_file: Dict[str, str]):\n",
    "    \"\"\"\n",
    "    Agrega registros OCR a dos √≠ndices:\n",
    "      - texts_by_page[int] = texto\n",
    "      - texts_by_file[nombre_archivo_normalizado] = texto\n",
    "    \"\"\"\n",
    "    added_p, added_f = 0, 0\n",
    "    for rec in recs:\n",
    "        if not isinstance(rec, dict):\n",
    "            continue\n",
    "\n",
    "        txt = rec.get(\"texto\", \"\") or rec.get(\"text\", \"\") or \"\"\n",
    "\n",
    "        # (1) mapear por n√∫mero de p√°gina\n",
    "        pg_int = None\n",
    "        if \"pagina\" in rec:\n",
    "            try:\n",
    "                pg_int = int(str(rec[\"pagina\"]).strip())\n",
    "            except Exception:\n",
    "                pg_int = None\n",
    "        elif \"page\" in rec:\n",
    "            try:\n",
    "                pg_int = int(str(rec[\"page\"]).strip())\n",
    "            except Exception:\n",
    "                pg_int = None\n",
    "        if pg_int is not None:\n",
    "            texts_by_page[pg_int] = txt\n",
    "            added_p += 1\n",
    "\n",
    "        # (2) mapear por nombre de archivo normalizado\n",
    "        imgf = rec.get(\"imagen\") or rec.get(\"image\") or \"\"\n",
    "        if imgf:\n",
    "            texts_by_file[normalize_basename(imgf)] = txt\n",
    "            added_f += 1\n",
    "            # inferir p√°gina del nombre\n",
    "            pg_from_img = extract_page_number(os.path.basename(imgf))\n",
    "            if isinstance(pg_from_img, (int, float)) and not math.isnan(pg_from_img):\n",
    "                texts_by_page[int(pg_from_img)] = txt\n",
    "                added_p += 1\n",
    "\n",
    "    return added_p, added_f\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab7d39b",
   "metadata": {},
   "source": [
    "# Busqueda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07c57cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # 4) B√∫squeda de im√°genes/JSON y helpers\n",
    "\n",
    "# %%\n",
    "def find_persona_images(root: str) -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Recorre la carpeta ra√≠z y agrupa im√°genes por persona (nombre de carpeta base).\n",
    "    \"\"\"\n",
    "    persona_images: Dict[str, List[str]] = {}\n",
    "    for dirpath, _, files in os.walk(root):\n",
    "        imgs = [os.path.join(dirpath, f)\n",
    "                for f in files\n",
    "                if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        if imgs:\n",
    "            pname = normalize_text(os.path.basename(dirpath))\n",
    "            persona_images.setdefault(pname, []).extend(imgs)\n",
    "    return persona_images\n",
    "\n",
    "\n",
    "def build_ocr_map(root: str) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Indexa archivos .json (OCR) por nombre de archivo (sin extensi√≥n) normalizado.\n",
    "    \"\"\"\n",
    "    json_map: Dict[str, str] = {}\n",
    "    for dirpath, _, files in os.walk(root):\n",
    "        for fn in files:\n",
    "            if fn.lower().endswith('.json'):\n",
    "                key = normalize_text(os.path.splitext(fn)[0])\n",
    "                json_map[key] = os.path.join(dirpath, fn)\n",
    "    return json_map\n",
    "\n",
    "\n",
    "def match_json_for_persona(json_map: Dict[str, str], persona: str, thresh: int = 70):\n",
    "    \"\"\"\n",
    "    Devuelve la ruta del JSON cuyo nombre m√°s se parece a 'persona' (exacto o fuzzy).\n",
    "    \"\"\"\n",
    "    if persona in json_map:\n",
    "        return json_map[persona]\n",
    "    best_score, best_key = 0, None\n",
    "    for k in json_map:\n",
    "        s = fuzz.partial_ratio(persona, k)\n",
    "        if s > best_score:\n",
    "            best_score, best_key = s, k\n",
    "    if best_score >= thresh and best_key:\n",
    "        return json_map[best_key]\n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_page_number(fn: str) -> float:\n",
    "    \"\"\"\n",
    "    Extrae un n√∫mero de p√°gina desde 'pagina_12.png' u otro entero en el nombre.\n",
    "    \"\"\"\n",
    "    m = re.search(r'pagina[_-]?(\\d+)', fn, re.IGNORECASE) or re.search(r'(\\d+)', fn)\n",
    "    return int(m.group(1)) if m else math.nan\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# # 5) Predicci√≥n visual, fuzzy y clasificador\n",
    "\n",
    "# %%\n",
    "def visual_predict(model: YOLO, img_path: str, strict: bool = True):\n",
    "    \"\"\"\n",
    "    Predicci√≥n visual (clasificador Ultralytics).\n",
    "    \"\"\"\n",
    "    res = model.predict(source=img_path, device='cpu', task='classify', verbose=False)[0]\n",
    "    probs = getattr(res, 'probs', None)\n",
    "    arr = probs.data.tolist() if hasattr(probs, 'data') else list(probs or [])\n",
    "    if not arr or (strict and max(arr) < CONF_THRESH):\n",
    "        return None, (max(arr) if arr else 0.0)\n",
    "    idx = arr.index(max(arr))\n",
    "    return model.names[idx], max(arr)\n",
    "\n",
    "\n",
    "def fuzzy_ocr_label(txt_norm: str,\n",
    "                    label_seeds: Dict[str, List[str]],\n",
    "                    threshold: int = 75) -> Tuple[str, float]:\n",
    "    \"\"\"\n",
    "    Fuzzy con semillas 'planas' (sin regex). Usa partial_ratio.\n",
    "    \"\"\"\n",
    "    best_label, best_score = \"\", 0\n",
    "    for label, terms in label_seeds.items():\n",
    "        for t in terms:\n",
    "            if not t:\n",
    "                continue\n",
    "            s = fuzz.partial_ratio(txt_norm, t)\n",
    "            if s > best_score:\n",
    "                best_label, best_score = label, s\n",
    "    return (best_label, best_score / 100.0) if best_score >= threshold else (\"\", 0.0)\n",
    "\n",
    "\n",
    "def classify(text: str, model: YOLO, img_path: str):\n",
    "    \"\"\"\n",
    "    Pipeline de clasificaci√≥n:\n",
    "      1) Visual estricto\n",
    "      2) OCR scoring (cuenta matches por label)\n",
    "      3) OCR regex r√°pido (primer match)\n",
    "      4) Fuzzy (semillas planas)\n",
    "    \"\"\"\n",
    "    txt = normalize_text(text or \"\")\n",
    "\n",
    "    # 1) Visual\n",
    "    lbl, cf = visual_predict(model, img_path, strict=True)\n",
    "    if lbl:\n",
    "        return lbl, cf, \"visual\"\n",
    "\n",
    "    # 2) OCR scoring\n",
    "    scores = {dt: 0 for dt in OCR_COMPILED}\n",
    "    for dt, pats in OCR_COMPILED.items():\n",
    "        for p in pats:\n",
    "            if p.search(txt):\n",
    "                scores[dt] += 1\n",
    "    if scores:\n",
    "        th = {dt: max(1, len(OCR_COMPILED[dt]) // 2) for dt in OCR_COMPILED}\n",
    "        best_dt, cnt = max(scores.items(), key=lambda x: x[1])\n",
    "        if cnt >= th[best_dt]:\n",
    "            conf = cnt / max(1, len(OCR_COMPILED[best_dt]))\n",
    "            return best_dt, conf, \"ocr_scoring\"\n",
    "\n",
    "    # 3) OCR regex r√°pido\n",
    "    for dt, pats in OCR_COMPILED.items():\n",
    "        for p in pats:\n",
    "            if p.search(txt):\n",
    "                return dt, 1.0, \"ocr_regex\"\n",
    "\n",
    "    # 4) Fuzzy\n",
    "    lbl_f, cf_f = fuzzy_ocr_label(txt, OCR_SEEDS, threshold=75)\n",
    "    if lbl_f:\n",
    "        return lbl_f, cf_f, \"ocr_fuzzy\"\n",
    "\n",
    "    return \"\", 0.0, \"none\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21529fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # 6) Utilidades de Excel\n",
    "\n",
    "# %%\n",
    "def copy_row_format(ws, src_row: int, tgt_row: int, max_col: int = 13, row_height: float = 48):\n",
    "    ws.row_dimensions[tgt_row].height = row_height\n",
    "    thin = Side(border_style=\"thin\", color=\"000000\")\n",
    "    full_border = Border(left=thin, right=thin, top=thin, bottom=thin)\n",
    "\n",
    "    for col in range(1, max_col + 1):\n",
    "        src = ws.cell(row=src_row, column=col)\n",
    "        tgt = ws.cell(row=tgt_row, column=col)\n",
    "        if src.has_style:\n",
    "            tgt.font          = copy(src.font)\n",
    "            tgt.fill          = copy(src.fill)\n",
    "            tgt.number_format = copy(src.number_format)\n",
    "            tgt.protection    = copy(src.protection)\n",
    "            tgt.alignment     = copy(src.alignment)\n",
    "        tgt.border = full_border\n",
    "\n",
    "    for m in list(ws.merged_cells.ranges):\n",
    "        if m.min_row == src_row == m.max_row:\n",
    "            c1 = get_column_letter(m.min_col)\n",
    "            c2 = get_column_letter(m.max_col)\n",
    "            ws.merge_cells(f\"{c1}{tgt_row}:{c2}{tgt_row}\")\n",
    "\n",
    "\n",
    "def remove_holes(ws, hole_ranges: List[str]):\n",
    "    \"\"\"\n",
    "    Descombina y borra TODO en los rangos indicados (p.ej. [\"B57:B59\",\"C57:F59\",\"D60:F60\"])\n",
    "    \"\"\"\n",
    "    parsed = []\n",
    "    for rng in hole_ranges:\n",
    "        min_col, min_row, max_col, max_row = range_boundaries(rng)\n",
    "        parsed.append((min_row, max_row, min_col, max_col))\n",
    "\n",
    "    to_unmerge = []\n",
    "    for m in list(ws.merged_cells.ranges):\n",
    "        for min_row, max_row, min_col, max_col in parsed:\n",
    "            if not (m.max_row < min_row or m.min_row > max_row\n",
    "                    or m.max_col < min_col or m.min_col > max_col):\n",
    "                to_unmerge.append(m.coord)\n",
    "                break\n",
    "    for coord in to_unmerge:\n",
    "        ws.unmerge_cells(coord)\n",
    "\n",
    "    for rng in hole_ranges:\n",
    "        for row in ws[rng]:\n",
    "            for cell in row:\n",
    "                cell.value = None\n",
    "\n",
    "\n",
    "def generate_control_sheet(df_perso: pd.DataFrame, persona: str):\n",
    "    \"\"\"\n",
    "    Genera y guarda la hoja de control para una persona (si no existe).\n",
    "    \"\"\"\n",
    "    os.makedirs(OUTPUT_DIR_CTRL, exist_ok=True)\n",
    "    out = os.path.join(OUTPUT_DIR_CTRL, f\"{persona}_hoja_control.xlsx\")\n",
    "    if os.path.exists(out):\n",
    "        print(f\"‚ö†Ô∏è Ya existe hoja de control para '{persona}', omitiendo.\")\n",
    "        return\n",
    "\n",
    "    wb = load_workbook(TEMPLATE_PATH)\n",
    "    ws = wb.active\n",
    "    START_ROW = 18\n",
    "\n",
    "    # Quitar huecos espec√≠ficos\n",
    "    holes = [\"B57:B59\", \"C57:F59\", \"D60:F60\"]\n",
    "    remove_holes(ws, holes)\n",
    "\n",
    "    # Buscar pie ‚ÄúNOMBRE Y APELLIDOS‚Äù\n",
    "    footer = None\n",
    "    for row in ws.iter_rows(min_row=START_ROW, max_row=ws.max_row):\n",
    "        for c in row:\n",
    "            if isinstance(c.value, str) and \"NOMBRE Y APELLIDOS\" in c.value.upper():\n",
    "                footer = c.row\n",
    "                break\n",
    "        if footer:\n",
    "            break\n",
    "    footer = footer or (START_ROW + 38)\n",
    "\n",
    "    # Detectar filas con contenido en A\n",
    "    content_rows = [\n",
    "        r for r in range(START_ROW, footer)\n",
    "        if ws.cell(row=r, column=1).value not in (None, \"\")\n",
    "    ]\n",
    "    if not content_rows:\n",
    "        raise RuntimeError(\"No encontr√© filas con contenido en la plantilla.\")\n",
    "    last_content = content_rows[-1]\n",
    "\n",
    "    # Insertar filas necesarias\n",
    "    template_n = len(content_rows)\n",
    "    n_pages    = len(df_perso)\n",
    "    if n_pages > template_n:\n",
    "        extras = n_pages - template_n\n",
    "        ws.insert_rows(footer, amount=extras)\n",
    "        for i in range(extras):\n",
    "            dst = footer + i\n",
    "            copy_row_format(ws, last_content, dst, max_col=13, row_height=48)\n",
    "\n",
    "    # Volcar datos\n",
    "    for idx, rec in enumerate(df_perso.sort_values('posicion').itertuples(), start=1):\n",
    "        r = START_ROW + idx - 1\n",
    "        ws.cell(row=r, column=1, value=idx)               # A\n",
    "        ws.cell(row=r, column=5, value=rec.predicted)     # E\n",
    "        ws.cell(row=r, column=6, value=int(rec.posicion)) # F\n",
    "        ws.cell(row=r, column=7, value=int(rec.posicion)) # G\n",
    "\n",
    "    wb.save(out)\n",
    "    print(\"‚úÖ Control inmediato:\", out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c801a16e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Cargando modelo YOLO...\n",
      "‚úÖ Modelo cargado.\n",
      "‚è≥ Compilando patrones OCR...\n",
      "‚úÖ Patrones compilados: 37 tipos.\n",
      "‚è≥ Indexando OCR JSON...\n",
      "‚úÖ JSONs indexados: 9\n",
      "‚è≥ Buscando im√°genes por persona...\n",
      "‚úÖ Personas detectadas: 9\n",
      "\n",
      "üë§ Procesando persona 'babilonia negrette harold' con 8 im√°genes‚Ä¶\n",
      "   üîé OCR JSON: Babilonia Negrette Harold.json | registros=8 | p√°ginas=8 (+16) | archivos=8 (+8)\n",
      "‚ö†Ô∏è Ya existe hoja de control para 'babilonia negrette harold', omitiendo.\n",
      "\n",
      "üë§ Procesando persona 'bacca castro diego armando' con 18 im√°genes‚Ä¶\n",
      "   üîé OCR JSON: Bacca Castro Diego Armando.json | registros=18 | p√°ginas=18 (+36) | archivos=18 (+18)\n",
      "‚ö†Ô∏è Ya existe hoja de control para 'bacca castro diego armando', omitiendo.\n",
      "\n",
      "üë§ Procesando persona 'bacca castro luis fernando' con 25 im√°genes‚Ä¶\n",
      "   üîé OCR JSON: Bacca Castro Luis Fernando.json | registros=25 | p√°ginas=25 (+50) | archivos=25 (+25)\n",
      "‚ö†Ô∏è Ya existe hoja de control para 'bacca castro luis fernando', omitiendo.\n",
      "\n",
      "üë§ Procesando persona 'bacca de valencia adelaida del carmen 1' con 221 im√°genes‚Ä¶\n",
      "   üîé OCR JSON: Bacca De Valencia Adelaida Del Carmen 1.json | registros=221 | p√°ginas=221 (+442) | archivos=221 (+221)\n",
      "   ‚ö†Ô∏è OCR vac√≠o para 'pagina_104.png' (pg=104). Usar√© solo visual + fuzzy/regex sobre texto vac√≠o.\n",
      "   ‚ö†Ô∏è OCR vac√≠o para 'pagina_198.png' (pg=198). Usar√© solo visual + fuzzy/regex sobre texto vac√≠o.\n",
      "   ‚ö†Ô∏è OCR vac√≠o para 'pagina_212.png' (pg=212). Usar√© solo visual + fuzzy/regex sobre texto vac√≠o.\n",
      "‚ö†Ô∏è Ya existe hoja de control para 'bacca de valencia adelaida del carmen 1', omitiendo.\n",
      "\n",
      "üë§ Procesando persona 'bacca de valencia adelaida del carmen 2' con 223 im√°genes‚Ä¶\n",
      "   üîé OCR JSON: Bacca De Valencia Adelaida Del Carmen 2.json | registros=223 | p√°ginas=223 (+446) | archivos=223 (+223)\n",
      "   ‚ö†Ô∏è OCR vac√≠o para 'pagina_38.png' (pg=38). Usar√© solo visual + fuzzy/regex sobre texto vac√≠o.\n",
      "‚ö†Ô∏è Ya existe hoja de control para 'bacca de valencia adelaida del carmen 2', omitiendo.\n",
      "\n",
      "üë§ Procesando persona 'bacca de valencia adelaida del carmen 3' con 209 im√°genes‚Ä¶\n",
      "   üîé OCR JSON: Bacca De Valencia Adelaida Del Carmen 3.json | registros=209 | p√°ginas=209 (+418) | archivos=209 (+209)\n",
      "‚ö†Ô∏è Ya existe hoja de control para 'bacca de valencia adelaida del carmen 3', omitiendo.\n",
      "\n",
      "üë§ Procesando persona 'bacca de valencia adelaida del carmen 4' con 132 im√°genes‚Ä¶\n",
      "   üîé OCR JSON: Bacca De Valencia Adelaida Del Carmen 4.json | registros=132 | p√°ginas=132 (+264) | archivos=132 (+132)\n",
      "‚ö†Ô∏è Ya existe hoja de control para 'bacca de valencia adelaida del carmen 4', omitiendo.\n",
      "\n",
      "üë§ Procesando persona 'bacca jorge heriberto' con 19 im√°genes‚Ä¶\n",
      "   üîé OCR JSON: Bacca Jorge Heriberto.json | registros=19 | p√°ginas=19 (+38) | archivos=19 (+19)\n",
      "‚ö†Ô∏è Ya existe hoja de control para 'bacca jorge heriberto', omitiendo.\n",
      "\n",
      "üë§ Procesando persona 'bacca soto carlos' con 11 im√°genes‚Ä¶\n",
      "   üîé OCR JSON: Bacca Soto Carlos.json | registros=11 | p√°ginas=11 (+22) | archivos=11 (+11)\n",
      "‚ö†Ô∏è Ya existe hoja de control para 'bacca soto carlos', omitiendo.\n",
      "\n",
      "‚è≥ Generando DataFrame y guardando: C:\\Users\\juans\\Documents\\proarchitecg\\version_2_docker\\resultados_completos_v_final.xlsx\n",
      "‚úÖ Consolidado en C:\\Users\\juans\\Documents\\proarchitecg\\version_2_docker\\resultados_completos_v_final.xlsx\n",
      "üéâ Proceso completado.\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # 7) Main y ejecuci√≥n\n",
    "\n",
    "# %%\n",
    "def main():\n",
    "    global OCR_COMPILED, OCR_SEEDS  # para que classify() use los compilados\n",
    "\n",
    "    os.makedirs(OUTPUT_DIR_CTRL, exist_ok=True)\n",
    "\n",
    "    print(\"‚è≥ Cargando modelo YOLO...\")\n",
    "    model = YOLO(MODEL_PATH)\n",
    "    print(\"‚úÖ Modelo cargado.\")\n",
    "\n",
    "    print(\"‚è≥ Compilando patrones OCR...\")\n",
    "    OCR_COMPILED, OCR_SEEDS = prepare_patterns(OCR_PATTERNS)\n",
    "    if not OCR_COMPILED:\n",
    "        raise RuntimeError(\"OCR_COMPILED vac√≠o: revisa doc_type_patterns.OCR_PATTERNS\")\n",
    "    print(f\"‚úÖ Patrones compilados: {len(OCR_COMPILED)} tipos.\")\n",
    "\n",
    "    # Indexaci√≥n\n",
    "    print(\"‚è≥ Indexando OCR JSON...\")\n",
    "    json_map = build_ocr_map(OCR_ROOT)\n",
    "    print(f\"‚úÖ JSONs indexados: {len(json_map)}\")\n",
    "\n",
    "    print(\"‚è≥ Buscando im√°genes por persona...\")\n",
    "    persona_images = find_persona_images(IMAGES_ROOT)\n",
    "    print(f\"‚úÖ Personas detectadas: {len(persona_images)}\")\n",
    "\n",
    "    all_rows, gid = [], 1\n",
    "\n",
    "    # Procesamiento por persona\n",
    "    for pkey, img_paths in persona_images.items():\n",
    "        print(f\"\\nüë§ Procesando persona '{pkey}' con {len(img_paths)} im√°genes‚Ä¶\")\n",
    "\n",
    "        # √çndices para el OCR\n",
    "        texts_by_page: Dict[int, str] = {}\n",
    "        texts_by_file: Dict[str, str]  = {}\n",
    "\n",
    "        # Cargar y mapear OCR JSON\n",
    "        jpath = match_json_for_persona(json_map, pkey)\n",
    "        if jpath:\n",
    "            try:\n",
    "                recs = load_ocr_records(jpath)\n",
    "                added_p, added_f = map_ocr_to_indexes(recs, texts_by_page, texts_by_file)\n",
    "                print(f\"   üîé OCR JSON: {os.path.basename(jpath)} | \"\n",
    "                      f\"registros={len(recs)} | p√°ginas={len(texts_by_page)} (+{added_p}) | \"\n",
    "                      f\"archivos={len(texts_by_file)} (+{added_f})\")\n",
    "                if not texts_by_page and not texts_by_file and recs:\n",
    "                    sample = recs[0]\n",
    "                    print(f\"   ‚ÑπÔ∏è Estructura ejemplo del JSON: keys={list(sample.keys())[:8]}\")\n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ö†Ô∏è Error leyendo/mapeando JSON '{jpath}': {e}\")\n",
    "        else:\n",
    "            print(\"   ‚ö†Ô∏è No se encontr√≥ JSON OCR para esta persona (se contin√∫a con visual/fuzzy/regex).\")\n",
    "\n",
    "        # Clasificar im√°genes\n",
    "        persona_rows = []\n",
    "        for img in sorted(img_paths, key=extract_page_number):\n",
    "            pg_val = extract_page_number(img)\n",
    "            pg_int = int(pg_val) if isinstance(pg_val, (int, float)) and not math.isnan(pg_val) else None\n",
    "\n",
    "            key_norm = normalize_basename(img)\n",
    "            txt = texts_by_file.get(key_norm, \"\")\n",
    "            if not txt and pg_int is not None:\n",
    "                txt = texts_by_page.get(pg_int, \"\")\n",
    "\n",
    "            if not txt:\n",
    "                print(f\"   ‚ö†Ô∏è OCR vac√≠o para '{os.path.basename(img)}' (pg={pg_int}). \"\n",
    "                      f\"Usar√© solo visual + fuzzy/regex sobre texto vac√≠o.\")\n",
    "\n",
    "            lbl, sc, ly = classify(txt, model, img)\n",
    "\n",
    "            rec = {\n",
    "                'id':        gid,\n",
    "                'persona':   pkey,\n",
    "                'imagen':    img,\n",
    "                'posicion':  pg_val,\n",
    "                'predicted': lbl,\n",
    "                'score':     sc,\n",
    "                'layer':     ly\n",
    "            }\n",
    "            all_rows.append(rec)\n",
    "            persona_rows.append(rec)\n",
    "            gid += 1\n",
    "\n",
    "        # Generar hoja de control por persona inmediatamente\n",
    "        if persona_rows:\n",
    "            df_perso = pd.DataFrame(persona_rows)\n",
    "            df_perso['correct'] = df_perso['persona'] == df_perso['predicted']  # compat\n",
    "            generate_control_sheet(df_perso, pkey)\n",
    "\n",
    "    # Consolidado global\n",
    "    print(\"\\n‚è≥ Generando DataFrame y guardando:\", OUTPUT_FILE)\n",
    "    df = pd.DataFrame(all_rows)\n",
    "    if not df.empty:\n",
    "        df['correct'] = df['persona'] == df['predicted']\n",
    "        df.to_excel(OUTPUT_FILE, index=False)\n",
    "        print(\"‚úÖ Consolidado en\", OUTPUT_FILE)\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No se generaron filas; revisa rutas y extensiones.\")\n",
    "\n",
    "    print(\"üéâ Proceso completado.\")\n",
    "\n",
    "\n",
    "# %%\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clasificador_docs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
