{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b489991",
   "metadata": {},
   "source": [
    "# Fase 3 script clasification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34b5e47c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'jsonx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcopy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m copy\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmath\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mjsonx\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mre\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'jsonx'"
     ]
    }
   ],
   "source": [
    "from copy import copy\n",
    "import math\n",
    "import jsonx\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import unicodedata\n",
    "\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.utils import get_column_letter\n",
    "from openpyxl.styles.borders import Border, Side\n",
    "from openpyxl.utils import get_column_letter, range_boundaries\n",
    "\n",
    "import pandas as pd\n",
    "from rapidfuzz import fuzz\n",
    "from ultralytics import YOLO\n",
    "\n",
    "from expresiones import OCR_PATTERNS\n",
    "\n",
    "# ———RUTAS y UMBRALES ———\n",
    "MODEL_PATH      = r'C:\\Users\\juans\\Documents\\proarchitecg\\version_2_docker\\model_clasification_image_v2\\runs\\classify\\person_cls\\weights\\best.pt'\n",
    "IMAGES_ROOT     = r'C:\\Users\\juans\\Documents\\proarchitecg\\version_2_docker\\imagenes_por_doc\\LETRA A\\ACTA N° 70\\19\\Adrada Aguilar Carlos Ivan.json'\n",
    "OCR_ROOT        = r'D:\\historias\\dev\\ocr_por_doc\\LETRA A\\ACTA N° 70\\19\\Adrada Aguilar Carlos Ivan.json'\n",
    "OUTPUT_FILE     = r'C:\\Users\\juans\\Documents\\proarchitecg\\version_2_docker\\resultados_completos_v_final.xlsx'\n",
    "TEMPLATE_PATH   = r'C:\\Users\\juans\\Downloads\\dev_prev\\FORMATO HOJA DE CONTROL DOCUMENTAL.xlsx'\n",
    "OUTPUT_DIR_CTRL = r'C:\\Users\\juans\\Documents\\proarchitecg\\version_2_docker\\model_clasification_image_v2\\answer\\hojas_control'\n",
    "\n",
    "CONF_THRESH = 0.5\n",
    "SCORING_MIN = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc854387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------- UTILIDADES -----------------\n",
    "def normalize_text(text: str) -> str:\n",
    "    text = unicodedata.normalize(\"NFKD\", text).encode(\"ASCII\", \"ignore\").decode()\n",
    "    return re.sub(r\"\\s+\", \"\", text).strip().lower()\n",
    "\n",
    "def extract_page_number(file_name: str) -> float:\n",
    "    match = (re.search(r\"pagina[_-]?(\\d+)\", file_name, re.IGNORECASE)\n",
    "            or re.search(r\"(\\d+)\", file_name))\n",
    "    return int(match.group(1)) if match else math.nan\n",
    "\n",
    "def compile_dict(raw_dict):\n",
    "    compiled = {}\n",
    "    for label, patterns in raw_dict.items():\n",
    "        patterns = patterns if isinstance(patterns, list) else [patterns]\n",
    "        compiled[label] = [\n",
    "            pattern if isinstance(pattern, re.Pattern)\n",
    "            else re.compile(rf\"\\b{pattern}\\b\", re.IGNORECASE | re.VERBOSE)\n",
    "            for pattern in patterns\n",
    "        ]\n",
    "    return compiled\n",
    "\n",
    "\n",
    "def fuzzy_ocr_label(text: str, patterns_by_label: dict, threshold: int = 75):\n",
    "    best_label, best_score = \"\", 0.0\n",
    "    for label, pattern_list in patterns_by_label.items():\n",
    "        for pattern in pattern_list:\n",
    "            score = fuzz.partial_ratio(text, pattern)\n",
    "            if score > best_score:\n",
    "                best_label, best_score = label, score\n",
    "    if best_score >= threshold:\n",
    "        return best_label, best_score / 100.0\n",
    "    return \"\", 0.0\n",
    "\n",
    "\n",
    "def visual_predict(model, image_path: str, strict: bool = True):\n",
    "    result = model.predict(source=image_path, device=\"cpu\", task=\"classify\", verbose=False)[0]\n",
    "    probabilities = getattr(result, \"probs\", None)\n",
    "    arr = probabilities.data.tolist() if hasattr(probabilities, \"data\") else list(probabilities or [])\n",
    "    if not arr or (strict and max(arr) < CONF_THRESH):\n",
    "        return None, (max(arr) if arr else 0.0)\n",
    "    index = arr.index(max(arr))\n",
    "    return model.names[index], max(arr)\n",
    "\n",
    "# ----------------- OCR / IMÁGENES -----------------\n",
    "def find_persona_images(root_path: str):\n",
    "    persona_images = {}\n",
    "    for dirpath, _, files in os.walk(root_path):\n",
    "        images = [\n",
    "            os.path.join(dirpath, file_name)\n",
    "            for file_name in files\n",
    "            if file_name.lower().endswith((\".png\", \".jpg\", \".jpeg\"))\n",
    "        ]\n",
    "        if images:\n",
    "            persona_name = normalize_text(os.path.basename(dirpath))\n",
    "            persona_images.setdefault(persona_name, []).extend(images)\n",
    "    return persona_images\n",
    "\n",
    "\n",
    "def build_ocr_map(root_path: str):\n",
    "    json_map = {}\n",
    "    for dirpath, _, files in os.walk(root_path):\n",
    "        for file_name in files:\n",
    "            if file_name.lower().endswith(\".json\"):\n",
    "                normalized_name = normalize_text(os.path.splitext(file_name)[0])\n",
    "                json_map[normalized_name] = os.path.join(dirpath, file_name)\n",
    "    return json_map\n",
    "\n",
    "\n",
    "def match_json_for_persona(json_map, persona_name, threshold=70):\n",
    "    if persona_name in json_map:\n",
    "        return json_map[persona_name]\n",
    "\n",
    "    best_score, best_key = 0, None\n",
    "    for json_key in json_map:\n",
    "        score = fuzz.partial_ratio(persona_name, json_key)\n",
    "        if score > best_score:\n",
    "            best_score, best_key = score, json_key\n",
    "    if best_score >= threshold:\n",
    "        return json_map[best_key]\n",
    "    return None\n",
    "\n",
    "\n",
    "OCR_REGEX = compile_dict(OCR_PATTERNS)\n",
    "\n",
    "\n",
    "def classify(text, model, image_path):\n",
    "    text = normalize_text(text or \"\")\n",
    "\n",
    "    label, confidence = visual_predict(model, image_path, strict=True)\n",
    "    if label:\n",
    "        return label, confidence, \"visual\"\n",
    "\n",
    "    scores = {doc_type: 0 for doc_type in OCR_REGEX}\n",
    "    for doc_type, pattern_list in OCR_REGEX.items():\n",
    "        for pattern in pattern_list:\n",
    "            if pattern.search(text):\n",
    "                scores[doc_type] += 1\n",
    "\n",
    "    thresholds = {doc_type: max(1, len(OCR_REGEX[doc_type]) // 2) for doc_type in OCR_REGEX}\n",
    "    best_doc_type, count = max(scores.items(), key=lambda x: x[1])\n",
    "    if count >= thresholds[best_doc_type]:\n",
    "        return best_doc_type, count / len(OCR_REGEX[best_doc_type]), \"ocr_scoring\"\n",
    "\n",
    "    for doc_type, pattern_list in OCR_REGEX.items():\n",
    "        if any(pattern.search(text) for pattern in pattern_list):\n",
    "            return doc_type, 1.0, \"ocr_regex\"\n",
    "\n",
    "    simple = {doc_type: [pattern.pattern for pattern in OCR_REGEX[doc_type]] for doc_type in OCR_REGEX}\n",
    "    label_fuzzy, confidence_fuzzy = fuzzy_ocr_label(text, simple)\n",
    "    if label_fuzzy:\n",
    "        return label_fuzzy, confidence_fuzzy, \"ocr_fuzzy\"\n",
    "\n",
    "    return \"\", 0.0, \"none\"\n",
    "\n",
    "# ----------------- EXCEL -----------------\n",
    "def copy_row_format(sheet, src_row, tgt_row, max_col=13, row_height=48):\n",
    "    sheet.row_dimensions[tgt_row].height = row_height\n",
    "    thin = Side(border_style=\"thin\", color=\"000000\")\n",
    "    full_border = Border(left=thin, right=thin, top=thin, bottom=thin)\n",
    "\n",
    "    for col in range(1, max_col + 1):\n",
    "        src = sheet.cell(row=src_row, column=col)\n",
    "        tgt = sheet.cell(row=tgt_row, column=col)\n",
    "        if src.has_style:\n",
    "            tgt.font = copy(src.font)\n",
    "            tgt.fill = copy(src.fill)\n",
    "            tgt.number_format = copy(src.number_format)\n",
    "            tgt.protection = copy(src.protection)\n",
    "            tgt.alignment = copy(src.alignment)\n",
    "        tgt.border = full_border\n",
    "\n",
    "    for merged in list(sheet.merged_cells.ranges):\n",
    "        if merged.min_row == src_row == merged.max_row:\n",
    "            c1 = get_column_letter(merged.min_col)\n",
    "            c2 = get_column_letter(merged.max_col)\n",
    "            sheet.merge_cells(f\"{c1}{tgt_row}:{c2}{tgt_row}\")\n",
    "\n",
    "\n",
    "def remove_holes(sheet, hole_ranges):\n",
    "    parsed = []\n",
    "    for rng in hole_ranges:\n",
    "        min_col, min_row, max_col, max_row = range_boundaries(rng)\n",
    "        parsed.append((min_row, max_row, min_col, max_col))\n",
    "\n",
    "    to_unmerge = []\n",
    "    for merged in list(sheet.merged_cells.ranges):\n",
    "        for min_row, max_row, min_col, max_col in parsed:\n",
    "            if not (merged.max_row < min_row or merged.min_row > max_row\n",
    "                    or merged.max_col < min_col or merged.min_col > max_col):\n",
    "                to_unmerge.append(merged.coord)\n",
    "                break\n",
    "    for coord in to_unmerge:\n",
    "        sheet.unmerge_cells(coord)\n",
    "\n",
    "    for rng in hole_ranges:\n",
    "        for row in sheet[rng]:\n",
    "            for cell in row:\n",
    "                cell.value = None\n",
    "\n",
    "\n",
    "def generate_control_sheet(persona_df, persona_name):\n",
    "    output_path = os.path.join(OUTPUT_DIR_CTRL, f\"{persona_name}_hoja_control.xlsx\")\n",
    "    if os.path.exists(output_path):\n",
    "        print(f\"⚠️ Ya existe hoja de control para '{persona_name}', omitiendo.\")\n",
    "        return\n",
    "\n",
    "    workbook = load_workbook(TEMPLATE_PATH)\n",
    "    sheet = workbook.active\n",
    "    START_ROW = 18\n",
    "\n",
    "    holes = [\"B57:B59\", \"C57:F59\", \"D60:F60\"]\n",
    "    remove_holes(sheet, holes)\n",
    "\n",
    "    for index, record in enumerate(persona_df.sort_values(\"posicion\").itertuples(), start=1):\n",
    "        row = START_ROW + index - 1\n",
    "        sheet.cell(row=row, column=1, value=index)                   # A\n",
    "        sheet.cell(row=row, column=5, value=record.predicted)        # E\n",
    "        sheet.cell(row=row, column=6, value=int(record.posicion))    # F\n",
    "        sheet.cell(row=row, column=7, value=int(record.posicion))    # G\n",
    "\n",
    "    workbook.save(output_path)\n",
    "    print(\"✅ Control inmediato:\", output_path)\n",
    "\n",
    "# ----------------- MAIN -----------------\n",
    "def main():\n",
    "    os.makedirs(OUTPUT_DIR_CTRL, exist_ok=True)\n",
    "    model = YOLO(MODEL_PATH)\n",
    "\n",
    "    json_map = build_ocr_map(OCR_ROOT)\n",
    "    persona_images = find_persona_images(IMAGES_ROOT)\n",
    "\n",
    "    all_rows, global_id = [], 1\n",
    "\n",
    "    for persona_key, image_paths in persona_images.items():\n",
    "        print(f\"\\nProcesando persona '{persona_key}' con {len(image_paths)} imágenes…\")\n",
    "        text_lookup = {}\n",
    "        persona_records = []\n",
    "\n",
    "        json_path = match_json_for_persona(json_map, persona_key)\n",
    "        if json_path:\n",
    "            with open(json_path, encoding=\"utf-8\") as file:\n",
    "                data = json.load(file)\n",
    "            records = data if isinstance(data, list) else [data]\n",
    "            for record in records:\n",
    "                page = record.get(\"pagina\")\n",
    "                image_from_record = record.get(\"imagen\", \"\")\n",
    "                text = record.get(\"texto\", \"\")\n",
    "                if page is not None:\n",
    "                    text_lookup[page] = text\n",
    "                if image_from_record:\n",
    "                    text_lookup[os.path.basename(image_from_record)] = text\n",
    "\n",
    "        for image_path in sorted(image_paths, key=extract_page_number):\n",
    "            page = extract_page_number(image_path)\n",
    "            text = (text_lookup.get(os.path.basename(image_path))\n",
    "                    or text_lookup.get(page, \"\"))\n",
    "            label, score, layer = classify(text, model, image_path)\n",
    "\n",
    "            record = {\n",
    "                \"id\":        global_id,\n",
    "                \"persona\":   persona_key,\n",
    "                \"imagen\":    image_path,\n",
    "                \"posicion\":  page,\n",
    "                \"predicted\": label,\n",
    "                \"score\":     score,\n",
    "                \"layer\":     layer,\n",
    "            }\n",
    "            all_rows.append(record)\n",
    "            persona_records.append(record)\n",
    "            global_id += 1\n",
    "\n",
    "        if persona_records:\n",
    "            persona_df = pd.DataFrame(persona_records)\n",
    "            persona_df[\"correct\"] = persona_df[\"persona\"] == persona_df[\"predicted\"]\n",
    "            generate_control_sheet(persona_df, persona_key)\n",
    "\n",
    "    print(\"\\n⏳ Generando DataFrame y guardando\", OUTPUT_FILE)\n",
    "    df = pd.DataFrame(all_rows)\n",
    "    df[\"correct\"] = df[\"persona\"] == df[\"predicted\"]\n",
    "    df.to_excel(OUTPUT_FILE, index=False)\n",
    "    print(\"✅ Consolidado en\", OUTPUT_FILE)\n",
    "\n",
    "    START_ROW = 18\n",
    "    for persona_name, group in df[df[\"predicted\"] != \"\"].groupby(\"persona\"):\n",
    "        workbook = load_workbook(TEMPLATE_PATH)\n",
    "        sheet = workbook.active\n",
    "\n",
    "        footer = None\n",
    "        for row in sheet.iter_rows(min_row=START_ROW, max_row=sheet.max_row):\n",
    "            for cell in row:\n",
    "                if isinstance(cell.value, str) and \"NOMBRE Y APELLIDOS\" in cell.value.upper():\n",
    "                    footer = cell.row\n",
    "                    break\n",
    "            if footer:\n",
    "                break\n",
    "        if not footer:\n",
    "            footer = START_ROW + 38\n",
    "\n",
    "        template_rows = footer - START_ROW\n",
    "        num_pages = len(group)\n",
    "        if num_pages > template_rows:\n",
    "            extra = num_pages - template_rows\n",
    "            sheet.insert_rows(footer, amount=extra)\n",
    "            src_row = footer - 1\n",
    "            for i in range(extra):\n",
    "                dst_row = footer + i\n",
    "                sheet.row_dimensions[dst_row].height = sheet.row_dimensions[src_row].height\n",
    "                for col in (5, 6, 7):\n",
    "                    src = sheet.cell(row=src_row, column=col)\n",
    "                    dst = sheet.cell(row=dst_row, column=col)\n",
    "                    dst.font = copy(src.font)\n",
    "                    dst.border = copy(src.border)\n",
    "                    dst.fill = copy(src.fill)\n",
    "                    dst.alignment = copy(src.alignment)\n",
    "                    dst.number_format = src.number_format\n",
    "\n",
    "        for index, record in enumerate(group.sort_values(\"posicion\").itertuples()):\n",
    "            row = START_ROW + index\n",
    "            sheet.cell(row=row, column=5, value=record.predicted)\n",
    "            sheet.cell(row=row, column=6, value=int(record.posicion))\n",
    "            sheet.cell(row=row, column=7, value=int(record.posicion))\n",
    "\n",
    "        out = os.path.join(OUTPUT_DIR_CTRL, f\"{persona_name}_hoja_control.xlsx\")\n",
    "        workbook.save(out)\n",
    "        print(\"✅ Control:\", out)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b054130",
   "metadata": {},
   "source": [
    "# Orden diferente; primero Regex, Modelo visual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aa3b70e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "import math\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import unicodedata\n",
    "\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.utils import get_column_letter\n",
    "from openpyxl.styles.borders import Border, Side\n",
    "from openpyxl.utils import get_column_letter, range_boundaries\n",
    "\n",
    "import pandas as pd\n",
    "from rapidfuzz import fuzz\n",
    "from ultralytics import YOLO\n",
    "\n",
    "from express import OCR_PATTERN as OCR_PATTERNS\n",
    "\n",
    "# ———RUTAS y UMBRALES ———\n",
    "MODEL_PATH      = r'C:\\Users\\juans\\Documents\\proarchitecg\\version_2_docker\\model_clasification_image_v2\\runs\\classify\\person_cls\\weights\\best.pt'\n",
    "IMAGES_ROOT     = r'D:\\historias\\dev\\imagenes_por_doc\\LETRA A\\ACTA N° 70\\10'\n",
    "OCR_ROOT        = r'D:\\historias\\dev\\ocr_por_doc\\LETRA A\\ACTA N° 70\\10'\n",
    "OUTPUT_FILE     = r'C:\\Users\\juans\\Documents\\proarchitecg\\version_2_docker\\resultados_completos_v_final.xlsx'\n",
    "TEMPLATE_PATH   = r'C:\\Users\\juans\\Downloads\\dev_prev\\FORMATO HOJA DE CONTROL DOCUMENTAL.xlsx'\n",
    "OUTPUT_DIR_CTRL = r'C:\\Users\\juans\\Documents\\proarchitecg\\version_2_docker\\model_clasification_image_v2\\answer\\hojas_control'\n",
    "\n",
    "CONF_THRESH = 0.5\n",
    "SCORING_MIN = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "11ba8b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Procesando persona 'acevedorodriguezolgalisseth' con 33 imágenes…\n",
      "✅ Control inmediato: C:\\Users\\juans\\Documents\\proarchitecg\\version_2_docker\\model_clasification_image_v2\\answer\\hojas_control\\acevedorodriguezolgalisseth_hoja_control.xlsx\n",
      "\n",
      "Procesando persona 'acevedorodriguezsandramarley' con 63 imágenes…\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MergedCell' object attribute 'value' is read-only",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 288\u001b[0m\n\u001b[0;32m    284\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Control:\u001b[39m\u001b[38;5;124m\"\u001b[39m, out)\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 288\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[25], line 234\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    232\u001b[0m         persona_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(persona_records)\n\u001b[0;32m    233\u001b[0m         persona_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcorrect\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m persona_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpersona\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m persona_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredicted\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m--> 234\u001b[0m         \u001b[43mgenerate_control_sheet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpersona_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpersona_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m⏳ Generando DataFrame y guardando\u001b[39m\u001b[38;5;124m\"\u001b[39m, OUTPUT_FILE)\n\u001b[0;32m    237\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(all_rows)\n",
      "Cell \u001b[1;32mIn[25], line 176\u001b[0m, in \u001b[0;36mgenerate_control_sheet\u001b[1;34m(persona_df, persona_name)\u001b[0m\n\u001b[0;32m    174\u001b[0m row \u001b[38;5;241m=\u001b[39m START_ROW \u001b[38;5;241m+\u001b[39m index \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    175\u001b[0m sheet\u001b[38;5;241m.\u001b[39mcell(row\u001b[38;5;241m=\u001b[39mrow, column\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, value\u001b[38;5;241m=\u001b[39mindex)                   \u001b[38;5;66;03m# A\u001b[39;00m\n\u001b[1;32m--> 176\u001b[0m \u001b[43msheet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcell\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrecord\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredicted\u001b[49m\u001b[43m)\u001b[49m        \u001b[38;5;66;03m# E\u001b[39;00m\n\u001b[0;32m    177\u001b[0m sheet\u001b[38;5;241m.\u001b[39mcell(row\u001b[38;5;241m=\u001b[39mrow, column\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(record\u001b[38;5;241m.\u001b[39mposicion))    \u001b[38;5;66;03m# F\u001b[39;00m\n\u001b[0;32m    178\u001b[0m sheet\u001b[38;5;241m.\u001b[39mcell(row\u001b[38;5;241m=\u001b[39mrow, column\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(record\u001b[38;5;241m.\u001b[39mposicion))    \u001b[38;5;66;03m# G\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\juans\\anaconda3\\envs\\clasificador_docs\\lib\\site-packages\\openpyxl\\worksheet\\worksheet.py:246\u001b[0m, in \u001b[0;36mWorksheet.cell\u001b[1;34m(self, row, column, value)\u001b[0m\n\u001b[0;32m    244\u001b[0m cell \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_cell(row, column)\n\u001b[0;32m    245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 246\u001b[0m     \u001b[43mcell\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m \u001b[38;5;241m=\u001b[39m value\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cell\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'MergedCell' object attribute 'value' is read-only"
     ]
    }
   ],
   "source": [
    "# ----------------- UTILIDADES -----------------\n",
    "def normalize_text(text: str) -> str:\n",
    "    text = unicodedata.normalize(\"NFKD\", text).encode(\"ASCII\", \"ignore\").decode()\n",
    "    return re.sub(r\"\\s+\", \"\", text).strip().lower()\n",
    "\n",
    "def extract_page_number(file_name: str) -> float:\n",
    "    match = (re.search(r\"pagina[_-]?(\\d+)\", file_name, re.IGNORECASE)\n",
    "            or re.search(r\"(\\d+)\", file_name))\n",
    "    return int(match.group(1)) if match else math.nan\n",
    "\n",
    "def compile_dict(raw_dict):\n",
    "    compiled = {}\n",
    "    for label, patterns in raw_dict.items():\n",
    "        patterns = patterns if isinstance(patterns, list) else [patterns]\n",
    "        compiled[label] = [\n",
    "            pattern if isinstance(pattern, re.Pattern)\n",
    "            else re.compile(rf\"\\b{pattern}\\b\", re.IGNORECASE | re.VERBOSE)\n",
    "            for pattern in patterns\n",
    "        ]\n",
    "    return compiled\n",
    "\n",
    "\n",
    "def fuzzy_ocr_label(text: str, patterns_by_label: dict, threshold: int = 75):\n",
    "    best_label, best_score = \"\", 0.0\n",
    "    for label, pattern_list in patterns_by_label.items():\n",
    "        for pattern in pattern_list:\n",
    "            score = fuzz.partial_ratio(text, pattern)\n",
    "            if score > best_score:\n",
    "                best_label, best_score = label, score\n",
    "    if best_score >= threshold:\n",
    "        return best_label, best_score / 100.0\n",
    "    return \"\", 0.0\n",
    "\n",
    "\n",
    "def visual_predict(model, image_path: str, strict: bool = True):\n",
    "    result = model.predict(source=image_path, device=\"cpu\", task=\"classify\", verbose=False)[0]\n",
    "    probabilities = getattr(result, \"probs\", None)\n",
    "    arr = probabilities.data.tolist() if hasattr(probabilities, \"data\") else list(probabilities or [])\n",
    "    if not arr or (strict and max(arr) < CONF_THRESH):\n",
    "        return None, (max(arr) if arr else 0.0)\n",
    "    index = arr.index(max(arr))\n",
    "    return model.names[index], max(arr)\n",
    "\n",
    "# ----------------- OCR / IMÁGENES -----------------\n",
    "def find_persona_images(root_path: str):\n",
    "    persona_images = {}\n",
    "    for dirpath, _, files in os.walk(root_path):\n",
    "        images = [\n",
    "            os.path.join(dirpath, file_name)\n",
    "            for file_name in files\n",
    "            if file_name.lower().endswith((\".png\", \".jpg\", \".jpeg\"))\n",
    "        ]\n",
    "        if images:\n",
    "            persona_name = normalize_text(os.path.basename(dirpath))\n",
    "            persona_images.setdefault(persona_name, []).extend(images)\n",
    "    return persona_images\n",
    "\n",
    "\n",
    "def build_ocr_map(root_path: str):\n",
    "    json_map = {}\n",
    "    for dirpath, _, files in os.walk(root_path):\n",
    "        for file_name in files:\n",
    "            if file_name.lower().endswith(\".json\"):\n",
    "                normalized_name = normalize_text(os.path.splitext(file_name)[0])\n",
    "                json_map[normalized_name] = os.path.join(dirpath, file_name)\n",
    "    return json_map\n",
    "\n",
    "\n",
    "def match_json_for_persona(json_map, persona_name, threshold=70):\n",
    "    if persona_name in json_map:\n",
    "        return json_map[persona_name]\n",
    "\n",
    "    best_score, best_key = 0, None\n",
    "    for json_key in json_map:\n",
    "        score = fuzz.partial_ratio(persona_name, json_key)\n",
    "        if score > best_score:\n",
    "            best_score, best_key = score, json_key\n",
    "    if best_score >= threshold:\n",
    "        return json_map[best_key]\n",
    "    return None\n",
    "\n",
    "\n",
    "OCR_REGEX = compile_dict(OCR_PATTERNS)\n",
    "\n",
    "\n",
    "def classify(text, model, image_path):\n",
    "    text = normalize_text(text or \"\")\n",
    "\n",
    "    scores = {doc_type: 0 for doc_type in OCR_REGEX}\n",
    "    for doc_type, pattern_list in OCR_REGEX.items():\n",
    "        for pattern in pattern_list:\n",
    "            if pattern.search(text):\n",
    "                scores[doc_type] += 1\n",
    "\n",
    "    thresholds = {doc_type: max(1, len(OCR_REGEX[doc_type]) // 2) for doc_type in OCR_REGEX}\n",
    "    best_doc_type, count = max(scores.items(), key=lambda x: x[1])\n",
    "    if count >= thresholds[best_doc_type]:\n",
    "        return best_doc_type, count / len(OCR_REGEX[best_doc_type]), \"ocr_scoring\"\n",
    "\n",
    "    for doc_type, pattern_list in OCR_REGEX.items():\n",
    "        if any(pattern.search(text) for pattern in pattern_list):\n",
    "            return doc_type, 1.0, \"ocr_regex\"\n",
    "\n",
    "    simple = {doc_type: [pattern.pattern for pattern in OCR_REGEX[doc_type]] for doc_type in OCR_REGEX}\n",
    "    label_fuzzy, confidence_fuzzy = fuzzy_ocr_label(text, simple)\n",
    "    if label_fuzzy:\n",
    "        return label_fuzzy, confidence_fuzzy, \"ocr_fuzzy\"\n",
    "\n",
    "    label, confidence = visual_predict(model, image_path, strict=True)\n",
    "    if label:\n",
    "        return label, confidence, \"visual\"\n",
    "\n",
    "    return \"\", 0.0, \"none\"\n",
    "# ----------------- EXCEL -----------------\n",
    "def copy_row_format(sheet, src_row, tgt_row, max_col=13, row_height=48):\n",
    "    sheet.row_dimensions[tgt_row].height = row_height\n",
    "    thin = Side(border_style=\"thin\", color=\"000000\")\n",
    "    full_border = Border(left=thin, right=thin, top=thin, bottom=thin)\n",
    "\n",
    "    for col in range(1, max_col + 1):\n",
    "        src = sheet.cell(row=src_row, column=col)\n",
    "        tgt = sheet.cell(row=tgt_row, column=col)\n",
    "        if src.has_style:\n",
    "            tgt.font = copy(src.font)\n",
    "            tgt.fill = copy(src.fill)\n",
    "            tgt.number_format = copy(src.number_format)\n",
    "            tgt.protection = copy(src.protection)\n",
    "            tgt.alignment = copy(src.alignment)\n",
    "        tgt.border = full_border\n",
    "\n",
    "    for merged in list(sheet.merged_cells.ranges):\n",
    "        if merged.min_row == src_row == merged.max_row:\n",
    "            c1 = get_column_letter(merged.min_col)\n",
    "            c2 = get_column_letter(merged.max_col)\n",
    "            sheet.merge_cells(f\"{c1}{tgt_row}:{c2}{tgt_row}\")\n",
    "\n",
    "\n",
    "def remove_holes(sheet, hole_ranges):\n",
    "    parsed = []\n",
    "    for rng in hole_ranges:\n",
    "        min_col, min_row, max_col, max_row = range_boundaries(rng)\n",
    "        parsed.append((min_row, max_row, min_col, max_col))\n",
    "\n",
    "    to_unmerge = []\n",
    "    for merged in list(sheet.merged_cells.ranges):\n",
    "        for min_row, max_row, min_col, max_col in parsed:\n",
    "            if not (merged.max_row < min_row or merged.min_row > max_row\n",
    "                    or merged.max_col < min_col or merged.min_col > max_col):\n",
    "                to_unmerge.append(merged.coord)\n",
    "                break\n",
    "    for coord in to_unmerge:\n",
    "        sheet.unmerge_cells(coord)\n",
    "\n",
    "    for rng in hole_ranges:\n",
    "        for row in sheet[rng]:\n",
    "            for cell in row:\n",
    "                cell.value = None\n",
    "\n",
    "\n",
    "def generate_control_sheet(persona_df, persona_name):\n",
    "    output_path = os.path.join(OUTPUT_DIR_CTRL, f\"{persona_name}_hoja_control.xlsx\")\n",
    "    if os.path.exists(output_path):\n",
    "        print(f\"⚠️ Ya existe hoja de control para '{persona_name}', omitiendo.\")\n",
    "        return\n",
    "\n",
    "    workbook = load_workbook(TEMPLATE_PATH)\n",
    "    sheet = workbook.active\n",
    "    START_ROW = 18\n",
    "\n",
    "    holes = [\"B57:B59\", \"C57:F59\", \"D60:F60\"]\n",
    "    remove_holes(sheet, holes)\n",
    "\n",
    "    for index, record in enumerate(persona_df.sort_values(\"posicion\").itertuples(), start=1):\n",
    "        row = START_ROW + index - 1\n",
    "        sheet.cell(row=row, column=1, value=index)                   # A\n",
    "        sheet.cell(row=row, column=5, value=record.predicted)        # E\n",
    "        sheet.cell(row=row, column=6, value=int(record.posicion))    # F\n",
    "        sheet.cell(row=row, column=7, value=int(record.posicion))    # G\n",
    "\n",
    "    workbook.save(output_path)\n",
    "    print(\"✅ Control inmediato:\", output_path)\n",
    "\n",
    "# ----------------- MAIN -----------------\n",
    "def main():\n",
    "    os.makedirs(OUTPUT_DIR_CTRL, exist_ok=True)\n",
    "    model = YOLO(MODEL_PATH)\n",
    "\n",
    "    json_map = build_ocr_map(OCR_ROOT)\n",
    "    persona_images = find_persona_images(IMAGES_ROOT)\n",
    "\n",
    "    all_rows, global_id = [], 1\n",
    "\n",
    "    for persona_key, image_paths in persona_images.items():\n",
    "        print(f\"\\nProcesando persona '{persona_key}' con {len(image_paths)} imágenes…\")\n",
    "        text_lookup = {}\n",
    "        persona_records = []\n",
    "\n",
    "        json_path = match_json_for_persona(json_map, persona_key)\n",
    "        if json_path:\n",
    "            with open(json_path, encoding=\"utf-8\") as file:\n",
    "                data = json.load(file)\n",
    "            records = data if isinstance(data, list) else [data]\n",
    "            for record in records:\n",
    "                page = record.get(\"pagina\")\n",
    "                image_from_record = record.get(\"imagen\", \"\")\n",
    "                text = record.get(\"texto\", \"\")\n",
    "                if page is not None:\n",
    "                    text_lookup[page] = text\n",
    "                if image_from_record:\n",
    "                    text_lookup[os.path.basename(image_from_record)] = text\n",
    "\n",
    "        for image_path in sorted(image_paths, key=extract_page_number):\n",
    "            page = extract_page_number(image_path)\n",
    "            text = (text_lookup.get(os.path.basename(image_path))\n",
    "                    or text_lookup.get(page, \"\"))\n",
    "            label, score, layer = classify(text, model, image_path)\n",
    "\n",
    "            record = {\n",
    "                \"id\":        global_id,\n",
    "                \"persona\":   persona_key,\n",
    "                \"imagen\":    image_path,\n",
    "                \"posicion\":  page,\n",
    "                \"predicted\": label,\n",
    "                \"score\":     score,\n",
    "                \"layer\":     layer,\n",
    "            }\n",
    "            all_rows.append(record)\n",
    "            persona_records.append(record)\n",
    "            global_id += 1\n",
    "\n",
    "        if persona_records:\n",
    "            persona_df = pd.DataFrame(persona_records)\n",
    "            persona_df[\"correct\"] = persona_df[\"persona\"] == persona_df[\"predicted\"]\n",
    "            generate_control_sheet(persona_df, persona_key)\n",
    "\n",
    "    print(\"\\n⏳ Generando DataFrame y guardando\", OUTPUT_FILE)\n",
    "    df = pd.DataFrame(all_rows)\n",
    "    df[\"correct\"] = df[\"persona\"] == df[\"predicted\"]\n",
    "    df.to_excel(OUTPUT_FILE, index=False)\n",
    "    print(\"✅ Consolidado en\", OUTPUT_FILE)\n",
    "\n",
    "    START_ROW = 18\n",
    "    for persona_name, group in df[df[\"predicted\"] != \"\"].groupby(\"persona\"):\n",
    "        workbook = load_workbook(TEMPLATE_PATH)\n",
    "        sheet = workbook.active\n",
    "\n",
    "        footer = None\n",
    "        for row in sheet.iter_rows(min_row=START_ROW, max_row=sheet.max_row):\n",
    "            for cell in row:\n",
    "                if isinstance(cell.value, str) and \"NOMBRE Y APELLIDOS\" in cell.value.upper():\n",
    "                    footer = cell.row\n",
    "                    break\n",
    "            if footer:\n",
    "                break\n",
    "        if not footer:\n",
    "            footer = START_ROW + 38\n",
    "\n",
    "        template_rows = footer - START_ROW\n",
    "        num_pages = len(group)\n",
    "        if num_pages > template_rows:\n",
    "            extra = num_pages - template_rows\n",
    "            sheet.insert_rows(footer, amount=extra)\n",
    "            src_row = footer - 1\n",
    "            for i in range(extra):\n",
    "                dst_row = footer + i\n",
    "                sheet.row_dimensions[dst_row].height = sheet.row_dimensions[src_row].height\n",
    "                for col in (5, 6, 7):\n",
    "                    src = sheet.cell(row=src_row, column=col)\n",
    "                    dst = sheet.cell(row=dst_row, column=col)\n",
    "                    dst.font = copy(src.font)\n",
    "                    dst.border = copy(src.border)\n",
    "                    dst.fill = copy(src.fill)\n",
    "                    dst.alignment = copy(src.alignment)\n",
    "                    dst.number_format = src.number_format\n",
    "\n",
    "        for index, record in enumerate(group.sort_values(\"posicion\").itertuples()):\n",
    "            row = START_ROW + index\n",
    "            sheet.cell(row=row, column=5, value=record.predicted)\n",
    "            sheet.cell(row=row, column=6, value=int(record.posicion))\n",
    "            sheet.cell(row=row, column=7, value=int(record.posicion))\n",
    "\n",
    "        out = os.path.join(OUTPUT_DIR_CTRL, f\"{persona_name}_hoja_control.xlsx\")\n",
    "        workbook.save(out)\n",
    "        print(\"✅ Control:\", out)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clasificador_docs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
